{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import numpy\n",
    "import torchfile\n",
    "import numpy as np\n",
    "import os, time\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Settings(object):\n",
    "    \"\"\"\n",
    "    This class holds all the information about the training process in a central and accesable place. \n",
    "\n",
    "    Attributes:\n",
    "        restore:        Indicates whether weights should be restored from an previos run\n",
    "        epochs:         Number of epochs\n",
    "        optimizer:      Specifies the kind of optimizer used for training the generator and the discriminator\n",
    "        batch_size:     Number of samples in a batch\n",
    "        run:            specifies the run\n",
    "        momentum:       Momentum used for Adam Optimizer\n",
    "        lr:             Learning rate\n",
    "        data_path:      Path to data folder\n",
    "        data_set:       Which data set should be used \n",
    "        keep_prop:      Keep probability for dropout layer\n",
    "        show_captions:  Boolean which determines if generated images are stored with text descriptions or without\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    restore    = False\n",
    "    epochs     = 1000\n",
    "    optimizer  = tf.train.AdamOptimizer()\n",
    "    batch_size = 64\n",
    "    run        = 3000\n",
    "    momentum   = 0.5\n",
    "    lr         = 0.0002\n",
    "    data_path  = \"/data\"\n",
    "    data_set   = \"flower_embeddings_custom.npy\"\n",
    "    keep_prop  = 0.5\n",
    "    show_captions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print iterations progress from https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "'''\n",
    "Loads the data set which is specified in the Settings\n",
    "\n",
    "Attributes:\n",
    "    directory      : Directory to the folder where the data ist stored\n",
    "    image_size     : The size to which all images in the data set get reshaped to \n",
    "    load in memory : Booelan that determines if all images of the data set are load into the working memory at the start\n",
    "                    (speeds up the get_batches function) \n",
    "    \n",
    "'''\n",
    "\n",
    "    #image size is the desired dimension of all images that are in the batch. The are downsampled by the data loader.\n",
    "    #load_into_memory specifies whether the downsampled images should be laoded into memory. This can speed up\n",
    "    #                 training significantly. But of course it's not possbile to very large datasets\n",
    "    def __init__(self, directory, image_size=64, load_into_memory=False):\n",
    "    '''\n",
    "    Constructor\n",
    "    '''\n",
    "        print(\"## BUILD DataLoader ##\")\n",
    "        self.load_into_memory = load_into_memory\n",
    "        self.directory = directory\n",
    "        self.image_size = image_size\n",
    "        self.data = self.load_binaries()\n",
    "        print(np.shape(self.data))\n",
    "        print(\"Nr. Trainingsamples: \" + str(np.shape(self.data)[0]))\n",
    "\n",
    "    def load_binaries(self):\n",
    "        try:\n",
    "            print(\"[Info] loading \" + self.directory + \"/\" + Settings.data_set)\n",
    "            data = np.load(self.directory + \"/\" + Settings.data_set)\n",
    "            if self.load_into_memory:\n",
    "                nr_images = np.shape(data)[0]\n",
    "                print(\"[Attention] Loading \"+str(nr_images)+\" images with dimensions \"+str(self.image_size)+\"x\"+str(self.image_size)+\"x3 into memory!!\")\n",
    "                printProgressBar(0, nr_images, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "                for i, row in enumerate(data):\n",
    "                    img_file = row[1]\n",
    "                    img_matrix = self.load_image(str(img_file))\n",
    "                    row[2] = img_matrix\n",
    "                    printProgressBar(i + 1, nr_images, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "            return data\n",
    "        except FileNotFoundError:\n",
    "            print(\"[Error] \"+ self.directory + \"/\" + Settings.data_set + \" not found!\")\n",
    "            sys.exit()\n",
    "\n",
    "        data = []\n",
    "        with open(self.directory + '/flowers_icml/trainvalclasses.txt') as file:\n",
    "            train_classes = file.readlines()\n",
    "        train_classes = [x.strip() for x in train_classes]\n",
    "\n",
    "        # go through all training classes to extract the captions\n",
    "        for c in train_classes:\n",
    "            #create a list of all files within the folder corresponding to the class/category\n",
    "            filelist = os.listdir(self.directory + '/flowers_icml/' + c)\n",
    "            #loop through all the files\n",
    "            for file in filelist:\n",
    "                #check if it's actually a t7 file. Only these contain the information about the images\n",
    "                if file.endswith(\".t7\"):\n",
    "                    # load the torchfile\n",
    "                    _tmp = torchfile.load(self.directory + '/flowers_icml/' + c + \"/\" + file)\n",
    "                    caption = _tmp.txt\n",
    "                    img_file = _tmp.img\n",
    "                    # add the caption the filename of the image to the loaded data\n",
    "                    img_matrix = self.load_image(str(img_file))\n",
    "                    data.append([caption, img_file, img_matrix])\n",
    "\n",
    "\n",
    "        print(np.shape(data))\n",
    "        print(\"return binaries\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def load_image(self, image_file):\n",
    "        img = skimage.io.imread(self.directory + \"/\" + image_file)\n",
    "        # GRAYSCALE\n",
    "        if len(img.shape) == 2:\n",
    "            print(\"GrayScale Image -- \" + image_file + \"\\n\")\n",
    "            img_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n",
    "            img_new[:,:,0] = img\n",
    "            img_new[:,:,1] = img\n",
    "            img_new[:,:,2] = img\n",
    "            img = img_new\n",
    "\n",
    "        img = skimage.transform.resize(img, (self.image_size, self.image_size),mode='reflect')\n",
    "        return img\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "    '''\n",
    "    Creates a batch of a specific size with captions, embeddings, and two types of images.\n",
    "    \n",
    "    @param:\n",
    "        batch_size     : number of samples \n",
    "    @returns:\n",
    "        text_captions  : some text descriptions\n",
    "        real_embeddings: the text descriptions in embedded form\n",
    "        real_images    : images that go along with the text captions\n",
    "        wrong_images   : images that do not fit to the text captions\n",
    "    '''\n",
    "        #total number of samples\n",
    "        n = np.shape(self.data)[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = n\n",
    "\n",
    "        # shuffle the data to get random batches\n",
    "        random_indeces = np.random.choice(n, n, replace = False)\n",
    "        data_pool = np.array(self.data)[random_indeces, :]\n",
    "\n",
    "        wrong_random_indices = np.random.choice(n, n, replace = False)\n",
    "        wrong_data_pool = np.array(self.data)[wrong_random_indices, :]\n",
    "\n",
    "        #create the batches\n",
    "        for i in range(n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "\n",
    "            wrong_batch = wrong_data_pool[on:off]\n",
    "            batch = data_pool[on:off]\n",
    "\n",
    "            for w_entry, entry in zip(wrong_batch, batch):\n",
    "\n",
    "                if not self.load_into_memory:\n",
    "                    _tmpw = self.load_image(str(w_entry[1]))\n",
    "                    w_entry[1] = _tmpw\n",
    "\n",
    "                    _tmp = self.load_image(str(entry[1]))\n",
    "                    entry[1] = _tmp\n",
    "                    ## Fill correct batch\n",
    "\n",
    "                # get 5 randomly selected captions for each image(out of 10 correct ones)\n",
    "                indecies_random_captions = np.random.choice(min(len(entry[0]), 5), 1, replace = False)\n",
    "                entry[0] = entry[0][indecies_random_captions]\n",
    "                entry[3] = entry[3][indecies_random_captions[0]]\n",
    "\n",
    "            real_embeddings = np.array([i[0] for i in batch[:,0]])\n",
    "            text_captions = np.array([i for i in batch[:,3]])\n",
    "\n",
    "            real_images = np.array([i for i in batch[:,2]])\n",
    "            wrong_images = np.array([i for i in wrong_batch[:,2]])\n",
    "\n",
    "            yield real_embeddings, real_images, wrong_images, text_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_test = time.time()\n",
    "\n",
    "\n",
    "# data_loader = DataLoader(\"data\", load_into_memory=True)\n",
    "# gen = data_loader.get_batches(10)\n",
    "# real_embeddings, real_images, wrong_images, captions = next(gen)\n",
    "# print(np.shape(real_embeddings))\n",
    "# print(np.shape(real_images))\n",
    "# print(np.shape(wrong_images))\n",
    "# print(np.shape(captions))\n",
    "\n",
    "# print(np.shape(real_embeddings[0]))\n",
    "\n",
    "\n",
    "\n",
    "# t_test = time.time() - t_test\n",
    "# minutes_t, seconds_t = divmod(t_test, 60)\n",
    "# print((\"-- Loaded Binaries in {0: .0f}m{1: .2f}s.\").format( minutes_t,seconds_t))\n",
    "# t_test = time.time()\n",
    "# idx = 1\n",
    "\n",
    "# t_total = time.time()\n",
    "\n",
    "# t_test = time.time()\n",
    "# for real_embeddings, real_images, wrong_images, captions in data_loader.get_batches(64):\n",
    "#     t_test = time.time() - t_test\n",
    "#     minutes_t, seconds_t = divmod(t_test, 60)\n",
    "#     print((\"-- Finished Batch #{0} in {1: .0f}m{2: .2f}s.\").format(idx +1, minutes_t,seconds_t))\n",
    "#     t_test = time.time()\n",
    "#     idx += 1\n",
    "\n",
    "    \n",
    "# t_total = time.time() - t_total\n",
    "# minutes_t, seconds_t = divmod(t_total, 60)\n",
    "   \n",
    "# print((\"-- Finished Epoch in {0: .0f}m{1: .2f}s.\").format(minutes_t,seconds_t))\n",
    "\n",
    "# t_test = time.time()\n",
    "# for real_embeddings, real_images, wrong_images, captions in data_loader.get_batches(64):\n",
    "#     t_test = time.time() - t_test\n",
    "#     minutes_t, seconds_t = divmod(t_test, 60)\n",
    "#     print((\"-- Finished Batch #{0} in {1: .0f}m{2: .2f}s.\").format(idx +1, minutes_t,seconds_t))\n",
    "#     t_test = time.time()\n",
    "#     idx += 1\n",
    "\n",
    "    \n",
    "# t_total = time.time() - t_total\n",
    "# minutes_t, seconds_t = divmod(t_total, 60)\n",
    "   \n",
    "# print((\"-- Finished Epoch in {0: .0f}m{1: .2f}s.\").format(minutes_t,seconds_t))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "##### Generative Adversarial Network  ##########################\n",
    "################################################################\n",
    "\n",
    "class GAN(object):\n",
    "    \"\"\"\n",
    "    This class creates a tensorflow graph for a Generative Adversarial Network conditioned on text embeddings.\n",
    "\n",
    "    Inorder to run all nodes in the tensorflow graph the following placeholders must be filled:\n",
    "    self.ist_training           : Boolean which displays if the network is in training mode or not, this information is important for batch normalization \n",
    "\n",
    "    self.z_vector               : Noise vector that is provided as input to the generator\n",
    "    self.text_embeddings        : Text embeddings that are provided as input to the Generator\n",
    "\n",
    "    self.text_embeddings_real   : Text embeddings provided to the Discriminator together with correct images\n",
    "    self.real_images            : Correct images presented with the real text embeddings\n",
    "\n",
    "    self.text_embeddings_wrong  : Embeddings from the training data that are coupled up with images that do not belong to the embeddings\n",
    "    self.wrong_images           : Images from the training data, that are presented to the Discriminator together with wrong embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Constructor\n",
    "        \"\"\"\n",
    "        print(\"\\n###################################\")\n",
    "        print(\"#           Build Network          #\")\n",
    "        print(\"###################################\\n\")\n",
    "        \n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default(): \n",
    "            self.build_graph()\n",
    "    \n",
    "\n",
    "    def build_graph(self):\n",
    "        '''\n",
    "        Calls the build Generator function. Concatinates the Generator and Discriminator by feeding the generated\n",
    "        images by the Generator into the Discriminator. Calls the build Discriminator function. Defines two \n",
    "        separate loss function one for the Discriminator and one for the Generator and two separate optimizer objects.\n",
    "    \n",
    "        '''\n",
    "        self.ist_training = tf.placeholder(tf.bool, None)\n",
    "        \n",
    "        #########Embedding Placeholders########\n",
    "        self.text_embeddings = tf.placeholder(tf.float32,shape=[None,1024], name=\"text_embeddings\")\n",
    "        self.text_embeddings_real = tf.placeholder(tf.float32,shape=[None,1024], name=\"real_text_embeddings\")\n",
    "        self.text_embeddings_wrong = tf.placeholder(tf.float32,shape=[None,1024], name=\"wrong_text_embeddings\")\n",
    "        \n",
    "        ###### Build Generator####\n",
    "        self.build_generator()\n",
    "        \n",
    "        ####### Placeholders for Images####\n",
    "        with tf.variable_scope(\"real_images\"):\n",
    "            self.real_images = batch_norm(tf.placeholder(tf.float32, shape=[Settings.batch_size,64, 64, 3], name=\"real_images\"),[0,1,2], self.ist_training)\n",
    "                                                                                                                    #64,64,3]))\n",
    "            \n",
    "        with tf.variable_scope(\"wrong_images\"):\n",
    "            self.wrong_images = batch_norm(tf.placeholder(tf.float32,shape=[Settings.batch_size,64, 64, 3], name=\"wrong_images\"), [0,1,2], self.ist_training)                                                                                                                          #64,64,3]))\n",
    "        \n",
    "        dis_real_logits, dis_real_sigmoid   = self.discriminator(self.real_images, self.text_embeddings_real)\n",
    "        dis_wrong_logits, dis_wrong_sigmoid = self.discriminator(self.wrong_images, self.text_embeddings_wrong, reuse = True)\n",
    "        dis_fake_logits, dis_fake_image     = self.discriminator(self.generated_images, self.text_embeddings, reuse = True)\n",
    "               \n",
    "        ######Build Optimizer######\n",
    "#         gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_fake_logits, labels=tf.ones_like(dis_fake_logits)))\n",
    "        \n",
    "#         dis_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_real_logits, labels=tf.ones_like(dis_real_logits)))\n",
    "#         dis_loss_wrong = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_wrong_logits, labels=tf.zeros_like(dis_wrong_logits)))\n",
    "#         dis_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_fake_logits, labels=tf.zeros_like(dis_fake_logits)))\n",
    "        \n",
    "#         dis_loss = dis_loss_real + ((dis_loss_wrong + dis_loss_fake) * 0.5)\n",
    "        \n",
    "        ####OTHER####\n",
    "        real_score  = tf.reduce_mean(dis_real_sigmoid)\n",
    "        wrong_score = tf.reduce_mean(dis_wrong_sigmoid)\n",
    "        fake_score  = tf.reduce_mean(dis_fake_image )\n",
    "        dis_loss = -((tf.log(real_score) + (tf.log(1 - wrong_score) + tf.log(1 - fake_score)) / 2))\n",
    "        gen_loss = -tf.log(fake_score)\n",
    "  \n",
    "        self.dis_summary = tf.summary.scalar('Discriminator_Loss',dis_loss)\n",
    "        self.gen_summary = tf.summary.scalar('Generator_Loss',gen_loss)\n",
    "        \n",
    "        ##########################variable lists#########################################\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        dis_variables = [var for var in trainable_variables if \"dis\" in var.name]\n",
    "        gen_variables = [var for var in trainable_variables if \"gen\" in var.name]\n",
    "\n",
    "        \n",
    "        #############################Optimizer#############\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            self.dis_optimizer = tf.train.AdamOptimizer(Settings.lr, beta1=Settings.momentum).minimize(dis_loss, var_list=dis_variables)\n",
    "            self.gen_optimizer = tf.train.AdamOptimizer(Settings.lr, beta1=Settings.momentum).minimize(gen_loss, var_list=gen_variables)\n",
    "\n",
    "        \n",
    "        \n",
    "    ###################################################\n",
    "    ######## BUILD GENERATOR##########################\n",
    "    def build_generator(self):\n",
    "        '''Builds the Generator\n",
    "    \n",
    "        First it reduces the given text embedding using a feed forward layer with 128 nodes. Then concatinates the reduced\n",
    "        embeddings with the noise vector z and reshapes it to 1024 feature maps of size 4x4. Then it upsamples the size of \n",
    "        the feature maps to 8x8, then 16x16, then 32x32 and then to 64x64. In each layer batch normaization is and \n",
    "        stride 2 deconvolution is used. Dropout is applied to layer two and four.\n",
    "    \n",
    "        '''\n",
    "        \n",
    "       ######## Inputs ##########\n",
    "        self.z_vector = tf.placeholder(tf.float32, shape=[None,100], name=\"z\")\n",
    "        with tf.variable_scope(\"gen_reduce_embeddings\"):\n",
    "            #####norm here#####\n",
    "            reduced_embeddings=feedforward_layer(self.text_embeddings,[1024,128],activation=lrelu, is_training=self.ist_training)\n",
    "            concat_input=tf.concat([self.z_vector,reduced_embeddings],1)\n",
    "        \n",
    "        \n",
    "        #####layer1######\n",
    "        with tf.variable_scope(\"gen_first_layer\"):\n",
    "            drive_first=feedforward_layer(concat_input,[228,4*4*1024], is_training=self.ist_training)\n",
    "            reshaped_first=tf.reshape(drive_first,[Settings.batch_size,4,4,1024])\n",
    "            output_first=tf.nn.relu(batch_norm(reshaped_first,[0,1,2], self.ist_training))\n",
    "       \n",
    "        ####layer2####\n",
    "        with tf.variable_scope(\"gen_second_layer\"):\n",
    "            #welche kernel grÃ¶ÃŸe??\n",
    "            #deconv_layer(input, target_shape, filter, strides, padding, bias_init, norm_axes=[0,1,2], normalize=False, activation=None)\n",
    "            d_second_layer = deconv_layer(output_first,[Settings.batch_size,8,8,512],[5,5,512,1024],[1,2,2,1],\"SAME\",normalize=True,activation=tf.nn.relu, is_training=self.ist_training)\n",
    "            second_layer=tf.nn.dropout(d_second_layer,Settings.keep_prop)\n",
    "        \n",
    "        #####layer3#####\n",
    "        with tf.variable_scope(\"gen_third_layer\"):\n",
    "            third_layer = deconv_layer(second_layer,[Settings.batch_size,16,16,256],[5,5,256,512], [1,2,2,1],\"SAME\", normalize=True, activation=tf.nn.relu, is_training=self.ist_training)\n",
    "        \n",
    "        ######layer4######\n",
    "        with tf.variable_scope(\"gen_fourth_layer\"):\n",
    "            d_fourth_layer=deconv_layer(third_layer,[Settings.batch_size,32,32,128],[5,5,128,256], [1,2,2,1],\"SAME\", normalize=True, activation=tf.nn.relu, is_training=self.ist_training)\n",
    "            fourth_layer=tf.nn.dropout(d_fourth_layer,Settings.keep_prop)\n",
    "        \n",
    "        #####layer5#####\n",
    "        with tf.variable_scope(\"gen_fifth_layer\"):\n",
    "            fifth_layer=deconv_layer(fourth_layer,[Settings.batch_size,64,64,3],[5,5,3,128], [1,2,2,1],\"SAME\",normalize=True, activation=tf.nn.tanh, is_training=self.ist_training)\n",
    "            ###VORSICHT wir haben es zur tanh geaendert\n",
    "        self.generated_images = (fifth_layer + 1) * 0.5\n",
    "        #return self.generated_images\n",
    "                                \n",
    "\n",
    "    def discriminator(self, discriminator_images, text_embeddings, reuse=False):\n",
    "        '''\n",
    "        Discriminator receives a batch of pictures of the same typ as input e.g. batch of generated images, batch of real images OR\n",
    "        batch of wrong images. In the fourth layer a batch of text embeddings get provided as information to the discriminator. The discriminaor \n",
    "        outputs the probabilty of the given images to be real and fitting to the provided text embeddings.\n",
    "    \n",
    "        @param:\n",
    "            disciminator_images: batch of images\n",
    "            text_embeddings    : batch of text_embeddings\n",
    "            reuse              : Boolean which determines if the variable_scope will be reused or not\n",
    "                            (needed because we are calling the same discriminator three times with the three typs of data)\n",
    "        @return:\n",
    "            state6             : logits of the last layer \n",
    "            tf.sigmoid(state6) : sigmoidal activation of the last layer\n",
    "        '''\n",
    "        \n",
    "        ############inputs############\n",
    "        with tf.variable_scope(\"dis_batch_norm\", reuse=reuse):\n",
    "            input_images = batch_norm(discriminator_images, [0,1,2], self.ist_training)\n",
    "        ## VORSICHT BATCHNORM HINZUGEFUEGT\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"dis_first_layer\", reuse=reuse):\n",
    "            #64x64x3\n",
    "            state_1 = conv2d_layer(input_images, filter=[5,5,3,64], strides=[1,2,2,1], padding=\"SAME\", normalize=True, activation=lrelu, is_training=self.ist_training)\n",
    "        with tf.variable_scope(\"dis_second_layer\", reuse=reuse):\n",
    "            #32x32x128\n",
    "            state_2 = conv2d_layer(state_1, filter=[5,5,64,32], strides=[1,2,2,1], padding=\"SAME\", normalize=True, activation=lrelu, is_training=self.ist_training)\n",
    "        with tf.variable_scope(\"dis_third_layer\", reuse=reuse):\n",
    "            #16x16x256\n",
    "            state_3 = conv2d_layer(state_2, filter=[5,5,32,16], strides=[1,2,2,1], padding=\"SAME\", normalize=True, activation=lrelu,  is_training=self.ist_training)\n",
    "        with tf.variable_scope(\"dis_fourth_layer\", reuse=reuse):\n",
    "            #8x8x512\n",
    "            state_4 = conv2d_layer(state_3, filter=[5,5,16,4], strides=[1,2,2,1], padding=\"SAME\", normalize=True, activation=lrelu,  is_training=self.ist_training)\n",
    "            #out: 4x4x1024\n",
    "\n",
    "        ####  ADD TEXT EMBEDDINGS TO NETWORK  ####\n",
    "        with tf.variable_scope(\"dis_reduce_embeddings\", reuse=reuse):\n",
    "            reduced_embeddings = feedforward_layer(text_embeddings, [1024,128], activation=tf.nn.relu,  is_training=self.ist_training)\n",
    "            reduced_embeddings = tf.expand_dims(reduced_embeddings,1)\n",
    "            reduced_embeddings = tf.expand_dims(reduced_embeddings,2)\n",
    "            tiled_embeddings = tf.tile(reduced_embeddings, [1,4,4,1], name='tile_embeddings')\n",
    "        \n",
    "        ### CONCAT TEXT EMBEDDINGS AND STATE_4  ###\n",
    "        \n",
    "        with tf.variable_scope(\"dis_concat_layer\", reuse=reuse):\n",
    "            state_4_concat = tf.concat([state_4, tiled_embeddings], 3)\n",
    "            \n",
    "        with tf.variable_scope(\"dis_fifth_layer\", reuse=reuse):\n",
    "            state_5 = conv2d_layer(state_4_concat, filter=[1,1,132,4], strides=[1,1,1,1], padding=\"SAME\", normalize=True, activation=tf.nn.relu, is_training=self.ist_training)\n",
    "            #out: 4x4x132 \n",
    "            ### hier normalizieren?? oder danach\n",
    "            state5_flat = tf.reshape(state_5, [Settings.batch_size, -1])\n",
    "            ####oder hier??\n",
    "        with tf.variable_scope(\"dis_sixth_layer\", reuse=reuse):\n",
    "            #TODO PUT ACTIVATION FUNTION BACK\n",
    "            state_6 = feedforward_layer(state5_flat, [64, 1], norm_axes=[0], normalize=False, activation=None, is_training=self.ist_training)\n",
    "        \n",
    "        self.dis_out = state_6\n",
    "        \n",
    "        return state_6, tf.nn.sigmoid(state_6)\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "def conv2d_layer(input, filter, strides, padding, bias_init=0.0, norm_axes=[0,1,2], normalize=False, activation=None, is_training=None):\n",
    "    depth = input.shape[-1]\n",
    "    fan_in = int(input.shape[1] * input.shape[2])\n",
    "    \n",
    "    if activation == tf.nn.relu or activation== lrelu:\n",
    "        var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "    else:\n",
    "        var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "    W = tf.get_variable('weights', filter, initializer=var_init)\n",
    "    #variable summaries\n",
    "    b = tf.get_variable('biases', filter[-1], initializer=tf.constant_initializer(bias_init))\n",
    "    #variable summaries\n",
    "\n",
    "    state = tf.nn.conv2d(input,W, strides, padding) + b\n",
    "    #state_depth=state.shape[-1]\n",
    "\n",
    "    if normalize:\n",
    "        state=batch_norm(state,norm_axes, is_training)\n",
    "         \n",
    "    conv_out = state\n",
    "\n",
    "    if not(activation is None):\n",
    "        conv_out = activation(state)\n",
    "    \n",
    "    return conv_out\n",
    "\n",
    "def deconv_layer(input, target_shape, filter, strides, padding, bias_init=0.0, norm_axes=[0,1,2], normalize=False, activation=None, is_training=None):\n",
    "    depth = input.shape[-1]\n",
    "    fan_in = int(input.shape[1] * input.shape[2])\n",
    "    \n",
    "    if activation == tf.nn.relu or activation == lrelu:\n",
    "        var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "    else:\n",
    "        var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "    \n",
    "    W = tf.get_variable('weights', [filter[0], filter[1], target_shape[-1], depth], initializer=var_init)\n",
    "    #variable summaries\n",
    "    b = tf.get_variable('biases', target_shape[-1], initializer=tf.constant_initializer(bias_init))\n",
    "    #variable summaries\n",
    "\n",
    "    print(str(np.shape(input)) + \"input shape\")\n",
    "    state = tf.nn.conv2d_transpose(input, W, target_shape, strides, padding) + b\n",
    "    #state_depth=state.shape[-1]\n",
    "\n",
    "    if normalize:\n",
    "        state=batch_norm(state,norm_axes, is_training)\n",
    "         \n",
    "    conv_out = state\n",
    "\n",
    "    if not(activation is None):\n",
    "        conv_out = activation(state)\n",
    "    \n",
    "    return conv_out\n",
    "\n",
    "def feedforward_layer(input, weights, bias_init=0.0, norm_axes=[0], normalize=False, activation=None, is_training=None):\n",
    "    depth= input.shape[-1]\n",
    "    fan_in = int(input.shape[-1])\n",
    "    \n",
    "    if activation == tf.nn.relu or activation == lrelu:\n",
    "        var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "    else:\n",
    "        var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "    \n",
    "    W = tf.get_variable('weights', weights, tf.float32,var_init)\n",
    "    #variable summaries\n",
    "    b = tf.get_variable('biases', weights[-1], initializer=tf.constant_initializer(bias_init))\n",
    "    #variable summaries\n",
    "\n",
    "    state = tf.matmul(input,W) + b\n",
    "    #state_depth=state.shape[-1]\n",
    "\n",
    "    if normalize:\n",
    "        state = batch_norm(state,norm_axes, is_training)\n",
    "          \n",
    "    ff_out = state\n",
    "\n",
    "    if not(activation is None):\n",
    "        ff_out = activation(state)\n",
    "\n",
    "    return ff_out\n",
    "\n",
    "def flatten(x):\n",
    "    size = int(np.prod(x.shape[1:]))\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n",
    "# def batch_norm(inp,norm_axes):\n",
    "#     depth = inp.shape[-1]\n",
    "#     epsilon = 1e-6\n",
    "#     mean, var = tf.nn.moments(inp, norm_axes)\n",
    "#     offset = tf.get_variable('offset1', depth, dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "#     scale = tf.get_variable('scale1', depth, dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n",
    "#     state = tf.nn.batch_normalization(inp, mean, var, offset, scale, epsilon)\n",
    "#     return state\n",
    "def _pop_batch_norm(x, pop_mean, pop_var, offset, scale):\n",
    "    return tf.nn.batch_normalization(x, pop_mean, pop_var, offset, scale, 1e-6)\n",
    "\n",
    "def _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale):\n",
    "    decay = 0.99\n",
    "    new_pop_mean = pop_mean * decay + mean * (1 - decay)\n",
    "    dependency_1 = tf.assign(pop_mean, pop_mean * decay + mean * (1 - decay))\n",
    "    dependency_2 = tf.assign(pop_var, pop_var * decay + var * (1 - decay))\n",
    "\n",
    "    with tf.control_dependencies([dependency_1, dependency_2]):\n",
    "        return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)\n",
    "\n",
    "def batch_norm(x, axes, is_training):\n",
    "    depth = x.shape[-1]\n",
    "    mean, var = tf.nn.moments(x, axes = axes)\n",
    "    \n",
    "    \n",
    "\n",
    "    var_init = tf.constant_initializer(0.0)\n",
    "    offset = tf.get_variable(\"offset\", depth, tf.float32, var_init)\n",
    "    var_init = tf.constant_initializer(1.0)\n",
    "    scale = tf.get_variable(\"scale\", depth, tf.float32, var_init)\n",
    "\n",
    "    pop_mean = tf.get_variable(\"pop_mean\", depth, initializer = tf.zeros_initializer(), trainable = False)\n",
    "    pop_var = tf.get_variable(\"pop_var\", depth, initializer = tf.ones_initializer(), trainable = False)\n",
    "    \n",
    "#     return _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale)\n",
    "    return tf.cond(\n",
    "        is_training,\n",
    "        lambda: _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale),\n",
    "        lambda: _pop_batch_norm(x, pop_mean, pop_var, offset, scale)\n",
    "    )\n",
    "\n",
    "def lrelu(x, alpha=0.2):\n",
    "    return tf.nn.relu(x) - alpha * tf.nn.relu(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,gen):\n",
    "    '''\n",
    "    Runs the tensorflow graph and evaluates the summary nodes, the optimizer nodes and the generated pictures node. Stores\n",
    "    the generated pictures and the weights of the session after each epoch.\n",
    "    @param:\n",
    "        net: the tensorflow graph \n",
    "        gen: the data generator    \n",
    "\n",
    "        Settings.epochs       : number of epochs\n",
    "        Settings.restore      : Boolean determines if weights from last session will be restored or not\n",
    "        Settings.show_captions: Boolean if generated should be stored with captions of without\n",
    "\n",
    "    '''\n",
    "    ######create folder to store generated images#######\n",
    "    if not os.path.exists(\"./generated_pictures/\"+str(Settings.run)):\n",
    "        os.makedirs(\"./generated_pictures/\"+str(Settings.run))\n",
    "    \n",
    "    ######### create Write for Tensorboard################\n",
    "    discrimiator_writer=tf.summary.FileWriter('./summary_test/'+str(Settings.run)+'/discriminator',tf.get_default_graph())\n",
    "    generator_writer=tf.summary.FileWriter('./summary_test/'+str(Settings.run)+'/generator')\n",
    "    \n",
    "    print(\"\\n###################################\")\n",
    "    print(\"#          Start Training         #\")\n",
    "    print(\"###################################\\n\")\n",
    "    \n",
    "    ########################SESSION############################\n",
    "    with tf.Session(graph=net.graph) as session:\n",
    "        saver = tf.train.Saver()\n",
    "        last_real_images=np.zeros([Settings.batch_size,64,64,3])\n",
    "        to_visualize=np.ndarray(shape=(42,64,64,3))\n",
    "        \n",
    "        ############RESTORE PARAMETERS IF WANTED#########################\n",
    "        if Settings.restore:\n",
    "            saver.restore(session, tf.train.latest_checkpoint('./store_weights/'+str(Settings.run-1)))\n",
    "        else:\n",
    "            session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #############Training#################################################################################################\n",
    "        step=0\n",
    "        for epoch in range(Settings.epochs):\n",
    "            t = time.time()\n",
    "            #############Training#################################################################################################\n",
    "            for real_embeddings, real_images, wrong_images, real_captions in gen.get_batches(Settings.batch_size):\n",
    "\n",
    "                z = np.random.normal(0 , 1 ,[ Settings.batch_size , 100 ] )\n",
    "                feed_dict = {net.z_vector: z,\n",
    "                             net.real_images: real_images,\n",
    "                             net.text_embeddings_real: real_embeddings,\n",
    "                             net.wrong_images: wrong_images,\n",
    "                             net.text_embeddings_wrong: real_embeddings,\n",
    "                             net.text_embeddings: real_embeddings,\n",
    "                             net.ist_training: True}\n",
    "    \n",
    "                generated_images, _, _, dis_sum, gen_sum = session.run([net.generated_images,net.dis_optimizer,net.gen_optimizer,net.dis_summary,net.gen_summary],\n",
    "                            feed_dict=feed_dict)\n",
    "\n",
    "                ##############Tensorboard summaries###############\n",
    "                discrimiator_writer.add_summary(dis_sum,step)\n",
    "                generator_writer.add_summary(gen_sum,step)\n",
    "                step += 1\n",
    "                last_real_images=real_images\n",
    "\n",
    "            #####################Save Session###########################\n",
    "            #saver.save(session, \"./stored\",1)\n",
    "            saver.save(session,\"./store_weights/\"+str(Settings.run)+\"/stored\",step)\n",
    "        \n",
    "        ##########################Store Visualizations###############################\n",
    "            if Settings.show_captions:\n",
    "                ################### shows generated pictures with captions ###########\n",
    "                fig = visual_with_captions(generated_images[0:8], real_captions[0:8])\n",
    "            else:\n",
    "                ####shows generated images and real images in one plot############\n",
    "                to_visualize[0:7]=generated_images[0:7]\n",
    "                to_visualize[7:14]=last_real_images[0:7]\n",
    "                to_visualize[14:21]=generated_images[7:14]\n",
    "                to_visualize[21:28]=last_real_images[7:14]\n",
    "                to_visualize[28:35]=generated_images[14:21]\n",
    "                to_visualize[35:42]=last_real_images[14:21]\n",
    "\n",
    "                fig=visual(to_visualize)   \n",
    "            plt.savefig(\"./generated_pictures/\"+str(Settings.run)+\"/\"+str(epoch)+\".png\")\n",
    "            plt.close(fig)\n",
    "            \n",
    "            t = time.time() - t\n",
    "            minutes, seconds = divmod(t, 60)\n",
    "            print((\"-- Finished Epoch #{0} in {1: .0f}m{2: .2f}s.\").format(epoch +1, minutes,seconds))\n",
    "\n",
    "#################################################################################\n",
    "############################### Visualization##################################\n",
    "##############################################################################\n",
    "#### call with images as batches e.g fig=visual(batch[0:39])\n",
    "#### saving the figure with:\n",
    "### plt.savefig('first_try.svg')\n",
    "\n",
    "#%matplotlib inline\n",
    "def visual(images, colums=7):\n",
    "    '''\n",
    "    Creates a figure of all given images \n",
    "    @param:\n",
    "        images : Images that should be shown in one figure \n",
    "        colums : Number of images in one row\n",
    "    @return:\n",
    "        fig: figure of plot of the provided images\n",
    "    '''    \n",
    "    \n",
    "    generated=False\n",
    "    if(np.shape(images[0]) == (64,64,3)):\n",
    "        generated = True\n",
    "        \n",
    "    nr_images = len(images)-len(images)%colums\n",
    "    row = nr_images/colums\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    \n",
    "    for i in range(nr_images):\n",
    "        ax=fig.add_subplot(row,colums,i+1)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        if generated:\n",
    "            imgplot=ax.imshow(images[i])\n",
    "        else:\n",
    "            imgplot=ax.imshow(images[i][1])\n",
    "            \n",
    "    return fig\n",
    "\n",
    "#### for the captions  hab ich noch nicht mit unseren captions probieren kÃ¶nnen.\n",
    "def visual_with_captions(images, captions):\n",
    "    '''\n",
    "    Create a figure of images and captions\n",
    "    @param:\n",
    "        images   : a array of images\n",
    "        captions : a list of captions related to the images\n",
    "    @return:\n",
    "        fig      : figure containing the images and the captions\n",
    "    '''\n",
    "    \n",
    "    captions = insert_breaks(captions,23)\n",
    "    length = len(images)-len(images)%4\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    fig.canvas.set_window_title(\"\")\n",
    "    i = 0\n",
    "    count = 0\n",
    "    \n",
    "    while count < length:\n",
    "        ax = fig.add_subplot(length/2,4,i+1)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        imgplot = ax.imshow(images[count])\n",
    "        ax = fig.add_subplot(length/2,4,i+2,frameon=False)\n",
    "        ax.text(-0.15,0.6,captions[count])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        i = i+2\n",
    "        count = count+1\n",
    "    return fig\n",
    "\n",
    "##inserts line breaks into each caption, inserts break at the position of the next space after\n",
    "#### the parameter number.\n",
    "def insert_breaks(captions, number):\n",
    "    ''' \n",
    "    Inserts spaces into the caption\n",
    "    @param:\n",
    "        captions : list of text descriptions\n",
    "        number   : indicates the postion of where the spaces get inserted into the captions\n",
    "    '''\n",
    "    for index in range(len(captions)):\n",
    "        lines=[]\n",
    "        for i in range(0, len(captions[index]), number):\n",
    "                lines.append(captions[index][i:i+number])\n",
    "        captions[index] =  '\\n'.join(lines)\n",
    "    return captions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## BUILD DataLoader ##\n",
      "[Info] loading data/flower_embeddings_custom.npy\n",
      "[Attention] Loading 4355 images with dimensions 64x64x3 into memory!!\n",
      "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "(4355, 4)\n",
      "Nr. Trainingsamples: 4355\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-275bc9f7914a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_into_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GAN' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    Main that created a dataloader and a tensorflow graph of an GAN and trains it accordingly to the parameters\n",
    "    specified in the unique Settings object.\n",
    "    '''\n",
    "    dl = DataLoader('data', load_into_memory=True)\n",
    "    net = GAN()\n",
    "    train(net, gen = dl)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(net,dl,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dl=DataLoader('.')\n",
    "gen_test = dl.get_batches(100)\n",
    "batch,wrong_batch=next(gen_test)\n",
    "print(np.shape(batch[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(batch[:,0][0]))\n",
    "a=batch[:,0]\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=batch[0:32]\n",
    "print(np.shape(a))\n",
    "################################need this to get rid of embediings!!!!!!!!!!!!!\n",
    "a=np.delete(a, 0, axis=1)\n",
    "print(np.shape(a))\n",
    "print(np.shape(a[1][0]))\n",
    "print(np.shape(generated_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader('data')\n",
    "generator = data_loader.get_batches(10)\n",
    "real_embeddings, real_images, wrong_embeddings, wrong_images, random_embeddings = next(generator)\n",
    "\n",
    "\n",
    "print(\"real_embeddings: \" + str(np.shape(real_embeddings)))\n",
    "print(\"real_images: \" + str(np.shape(real_images)))\n",
    "print(\"wrong_embeddings: \" + str(np.shape(wrong_embeddings)))\n",
    "print(\"wrong_images: \" + str(np.shape(wrong_images)))\n",
    "print(\"random_embeddings: \" + str(np.shape(random_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "ja = [i for i in range(30)]\n",
    "print(ja[0:10])\n",
    "print(ja[10:10*2])\n",
    "print(ja[10*2:10*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch #1 in  0m 0.30s.\n"
     ]
    }
   ],
   "source": [
    "#always feed real captions\n",
    "print((\"Finished Epoch #{0} in {1: .0f}m{2: .2f}s.\").format(1, 0.2,0.3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a3a8f6cd61e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     image_file =  join(data_dir, 'flowers/jpg/'+loaded_data['image_list'][idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     image_array = image_processing.load_image_array(image_file, image_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_data' is not defined"
     ]
    }
   ],
   "source": [
    "batch_no = 1\n",
    "batch_size = 64\n",
    "real_images = np.zeros((batch_size, 64, 64, 3))\n",
    "wrong_images = np.zeros((batch_size, 64, 64, 3))\n",
    "captions = np.zeros((batch_size, 1024))\n",
    "\n",
    "cnt = 0\n",
    "image_files = []\n",
    "for i in range(batch_no * batch_size, batch_no * batch_size + batch_size):\n",
    "    idx = i % len(loaded_data['image_list'])\n",
    "    image_file =  join(data_dir, 'flowers/jpg/'+loaded_data['image_list'][idx])\n",
    "    image_array = image_processing.load_image_array(image_file, image_size)\n",
    "    real_images[cnt,:,:,:] = image_array\n",
    "\n",
    "    # Improve this selection of wrong image\n",
    "    wrong_image_id = random.randint(0,len(loaded_data['image_list'])-1)\n",
    "    wrong_image_file =  join(data_dir, 'flowers/jpg/'+loaded_data['image_list'][wrong_image_id])\n",
    "    wrong_image_array = image_processing.load_image_array(wrong_image_file, image_size)\n",
    "    wrong_images[cnt, :,:,:] = wrong_image_array\n",
    "\n",
    "    random_caption = random.randint(0,4)\n",
    "    captions[cnt,:] = loaded_data['captions'][ loaded_data['image_list'][idx] ][ random_caption ][0:caption_vector_length]\n",
    "    image_files.append( image_file )\n",
    "    cnt += 1\n",
    "\n",
    "z_noise = np.random.uniform(-1, 1, [batch_size, z_dim])\n",
    "return real_images, wrong_images, captions, z_noise, image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "hello=[1,2,3,4]\n",
    "print(hello[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-test",
   "language": "python",
   "name": "py36-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
