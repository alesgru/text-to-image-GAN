{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import numpy\n",
    "import torchfile\n",
    "import numpy as np\n",
    "import os, time\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Settings(object):\n",
    "    restore    = False\n",
    "    n_epochs   = 1400\n",
    "    optimizer  = tf.train.AdamOptimizer()\n",
    "    batch_size = 64\n",
    "    run        = 2200\n",
    "    momentum   = 0.5\n",
    "    lr         = 0.0002\n",
    "    data_path  = \"/data\"\n",
    "    data_set   = \"flower_embeddings_custom.npy\"\n",
    "    keep_prop  = 0.5\n",
    "    show_captions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import:\n",
    "# import skimage\n",
    "# import skimage.io\n",
    "# import skimage.transform\n",
    "# import numpy\n",
    "# import torchfile\n",
    "# import numpy as np\n",
    "# import os\n",
    "#\n",
    "# Usage:\n",
    "# gen = loader.get_batches(n)\n",
    "# real_embeddings, real_images, wrong_embeddings, wrong_images, random_embeddings = next(gen)\n",
    "#\n",
    "# data structure:\n",
    "# ---data\n",
    "#      \\---flowers_icml (https://drive.google.com/file/d/0B0ywwgffWnLLMl9uOU91MV80cVU/view?usp=sharing) (only if flower_embeddings.npy not given)\n",
    "#      \\\n",
    "#      \\---flower_embeddings.npy\n",
    "#      \\\n",
    "#      \\---jpg (http://www.robots.ox.ac.uk/~vgg/data/flowers/102/) dataset images\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    \n",
    "    #image size is the dimensions of all images that are in the batch. The are downsampled here.\n",
    "    def __init__(self, directory, image_size=64):\n",
    "        self.directory = directory\n",
    "        self.image_size = image_size\n",
    "        self.data = self.load_binaries()\n",
    "        print(\"# Trainingsamples: \" + str(np.shape(self.data)[0]))\n",
    "        \n",
    "    def load_binaries(self):\n",
    "        \n",
    "        try:\n",
    "            print(\"-- Load /flower_embeddings_medium.npy --\")\n",
    "            return np.load(self.directory + '/flower_embeddings_medium.npy')\n",
    "        except Error:\n",
    "            pass\n",
    "        \n",
    "        data = []\n",
    "        with open(self.directory + '/flowers_icml/trainvalclasses.txt') as file:\n",
    "            train_classes = file.readlines()\n",
    "        train_classes = [x.strip() for x in train_classes] \n",
    "        \n",
    "        # go through all training classes to extract the captions\n",
    "        for c in train_classes:\n",
    "            #create a list of all files within the folder corresponding to the class/category\n",
    "            filelist = os.listdir(self.directory + '/flowers_icml/' + c)\n",
    "            #loop through all the files\n",
    "            for file in filelist:\n",
    "                #check if it's actually a t7 file. Only these contain the information about the images\n",
    "                if file.endswith(\".t7\"):\n",
    "                    # load the torchfile\n",
    "                    _tmp = torchfile.load(self.directory + '/flowers_icml/' + c + \"/\" + file)\n",
    "                    caption = _tmp.txt\n",
    "                    img_file = _tmp.img\n",
    "                    # add the caption the filename of the image to the loaded data\n",
    "                    data.append([caption, img_file])\n",
    "        return data\n",
    "        \n",
    "        \n",
    "    def load_image(self, image_file):\n",
    "        img = skimage.io.imread(self.directory + \"/\" + image_file)\n",
    "#         print(image_file)\n",
    "        # GRAYSCALE\n",
    "        if len(img.shape) == 2:\n",
    "            print(\"GrayScale Image -- \" + image_file + \"\\n\")\n",
    "            img_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n",
    "            img_new[:,:,0] = img\n",
    "            img_new[:,:,1] = img\n",
    "            img_new[:,:,2] = img\n",
    "            img = img_new\n",
    "            \n",
    "        img = skimage.transform.resize(img, (self.image_size, self.image_size))\n",
    "        return img\n",
    "        \n",
    "    def get_batches(self, batch_size):\n",
    "        #total number of samples\n",
    "        n = np.shape(self.data)[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = n\n",
    "          \n",
    "        # shuffle the data to get random batches\n",
    "        random_indices = np.random.choice(n, n, replace = False)\n",
    "        data_pool = np.array(self.data)[random_indices, :]\n",
    "        \n",
    "        wrong_random_indices = np.random.choice(n, n, replace = False)\n",
    "        wrong_data_pool = np.array(self.data)[wrong_random_indices, :]\n",
    "\n",
    "        #create the batches\n",
    "        for i in range(n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "            \n",
    "            wrong_batch = wrong_data_pool[on:off]\n",
    "            batch = data_pool[on:off]\n",
    "            for w_entry, entry in zip(wrong_batch, batch):\n",
    "                ## Fill wrong batch\n",
    "                # get 5 random wrong captions for a real image\n",
    "                random_images = np.random.choice(n, 1, replace = False)\n",
    "                wrong_captions = []\n",
    "                wrong_captions_images = np.array(self.data)[random_images, :]\n",
    "                counter = 0\n",
    "                i = 0\n",
    "                while i < np.shape(wrong_captions_images)[0]:\n",
    "                    if wrong_captions_images[i][1] == w_entry[1]:\n",
    "                        _random_images = np.random.choice(n, 1, replace = False) #ERROR mit der 1\n",
    "                        wrong_captions_images = np.array(self.data)[_random_images, :]\n",
    "\n",
    "                    else:\n",
    "                        random_index = np.random.choice(len(wrong_captions_images[i][0]), 1, replace = False)\n",
    "                        caption = wrong_captions_images[i][0][random_index]\n",
    "                        wrong_captions.append(caption[0])\n",
    "                        i += 1\n",
    "                \n",
    "                w_entry[0] = wrong_captions\n",
    "                \n",
    "                _tmpw = self.load_image(str(w_entry[1].decode(\"utf-8\")))\n",
    "                w_entry[1] = _tmpw  \n",
    "                \n",
    "                ## Fill correct batch\n",
    "                # get 5 randomly selected captions for each image(out of 10 correct ones)\n",
    "                indecies_random_captions = np.random.choice(len(entry[0]), 1, replace = False)\n",
    "                entry[0] = entry[0][indecies_random_captions]\n",
    "                      \n",
    "                #load the actual image from the file\n",
    "                _tmp = self.load_image(str(entry[1].decode(\"utf-8\")))\n",
    "                entry[1] = _tmp\n",
    "            \n",
    "            #create random embeddings for the generator\n",
    "            # shuffle the data to get random batches\n",
    "            random_indices_embeddings = np.random.choice(n, n, replace = False)\n",
    "            data_pool_embeddings = np.array(self.data)[random_indices, :]\n",
    "            \n",
    "            ## create the elements that are returned\n",
    "            random_embeddings = []\n",
    "            for embeddings in data_pool_embeddings[on:off][:,0]:\n",
    "                idx = np.random.choice(np.shape(embeddings)[0], 1)\n",
    "                random_embeddings.append(embeddings[idx][0])\n",
    "            \n",
    "            real_embeddings = np.array([i[0] for i in batch[:,0]])\n",
    "            real_images = np.array([i for i in batch[:,1]])\n",
    "\n",
    "            wrong_embeddings = np.array([i[0] for i in wrong_batch[:,0]])   \n",
    "            wrong_images = np.array([i for i in wrong_batch[:,1]])\n",
    "            \n",
    "            yield real_embeddings, real_images, wrong_embeddings, wrong_images, random_embeddings\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Import:\n",
    "# # import skimage\n",
    "# # import skimage.io\n",
    "# # import skimage.transform\n",
    "# # import numpy\n",
    "# # import torchfile\n",
    "# # import numpy as np\n",
    "# # import os\n",
    "# #\n",
    "# # Usage:\n",
    "# # gen = loader.get_batches(n)\n",
    "# # real_embeddings, real_images, wrong_embeddings, wrong_images, random_embeddings = next(gen)\n",
    "# #\n",
    "# # data structure:\n",
    "# # ---data\n",
    "# #      \\---flowers_icml (https://drive.google.com/file/d/0B0ywwgffWnLLMl9uOU91MV80cVU/view?usp=sharing) (only if flower_embeddings.npy not given)\n",
    "# #      \\\n",
    "# #      \\---flower_embeddings.npy\n",
    "# #      \\\n",
    "# #      \\---jpg (http://www.robots.ox.ac.uk/~vgg/data/flowers/102/) dataset images\n",
    "\n",
    "\n",
    "# class DataLoader:\n",
    "    \n",
    "#     #image size is the dimensions of all images that are in the batch. The are downsampled here.\n",
    "#     def __init__(self, directory, image_size=64):\n",
    "#         print(\"## BUILD DataLoader ##\")\n",
    "#         self.directory = directory\n",
    "#         self.image_size = image_size\n",
    "#         self.data = self.load_binaries()\n",
    "#         print(\"Nr. Trainingsamples: \" + str(np.shape(self.data)[0]))\n",
    "        \n",
    "#     def load_binaries(self):\n",
    "        \n",
    "#         try:\n",
    "#             print(\"-- Load /flower_embeddings_medium.npy --\")\n",
    "#             return np.load(self.directory + '/flower_embeddings_medium.npy')\n",
    "#         except Error:\n",
    "#             pass\n",
    "        \n",
    "#         data = []\n",
    "#         with open(self.directory + '/flowers_icml/trainvalclasses.txt') as file:\n",
    "#             train_classes = file.readlines()\n",
    "#         train_classes = [x.strip() for x in train_classes] \n",
    "        \n",
    "#         # go through all training classes to extract the captions\n",
    "#         for c in train_classes:\n",
    "#             #create a list of all files within the folder corresponding to the class/category\n",
    "#             filelist = os.listdir(self.directory + '/flowers_icml/' + c)\n",
    "#             #loop through all the files\n",
    "#             for file in filelist:\n",
    "#                 #check if it's actually a t7 file. Only these contain the information about the images\n",
    "#                 if file.endswith(\".t7\"):\n",
    "#                     # load the torchfile\n",
    "#                     _tmp = torchfile.load(self.directory + '/flowers_icml/' + c + \"/\" + file)\n",
    "#                     caption = _tmp.txt\n",
    "#                     img_file = _tmp.img\n",
    "#                     # add the caption the filename of the image to the loaded data\n",
    "#                     data.append([caption, img_file])\n",
    "#         return data\n",
    "        \n",
    "        \n",
    "#     def load_image(self, image_file):\n",
    "#         img = skimage.io.imread(self.directory + \"/\" + image_file)\n",
    "# #         print(image_file)\n",
    "#         # GRAYSCALE\n",
    "#         if len(img.shape) == 2:\n",
    "#             print(\"GrayScale Image -- \" + image_file + \"\\n\")\n",
    "#             img_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n",
    "#             img_new[:,:,0] = img\n",
    "#             img_new[:,:,1] = img\n",
    "#             img_new[:,:,2] = img\n",
    "#             img = img_new\n",
    "            \n",
    "#         img = skimage.transform.resize(img, (self.image_size, self.image_size))\n",
    "#         return img\n",
    "        \n",
    "#     def get_batches(self, batch_size):\n",
    "#         #total number of samples\n",
    "#         n = np.shape(self.data)[0]\n",
    "#         if batch_size <= 0:\n",
    "#             batch_size = n\n",
    "          \n",
    "#         # shuffle the data to get random batches\n",
    "#         random_indices = np.random.choice(n, n, replace = False)\n",
    "#         data_pool = np.array(self.data)[random_indices, :]\n",
    "        \n",
    "#         wrong_random_indices = np.random.choice(n, n, replace = False)\n",
    "#         wrong_data_pool = np.array(self.data)[wrong_random_indices, :]\n",
    "\n",
    "#         #create the batches\n",
    "#         for i in range(n // batch_size):\n",
    "#             on = i * batch_size\n",
    "#             off = on + batch_size\n",
    "            \n",
    "#             wrong_batch = wrong_data_pool[on:off]\n",
    "#             batch = data_pool[on:off]\n",
    "#             for w_entry, entry in zip(wrong_batch, batch):\n",
    "#                 ## Fill wrong batch\n",
    "#                 # get 5 random wrong captions for a real image\n",
    "#                 random_images = np.random.choice(n, 1, replace = False)\n",
    "#                 wrong_captions = []\n",
    "#                 wrong_captions_images = np.array(self.data)[random_images, :]\n",
    "#                 counter = 0\n",
    "#                 i = 0\n",
    "#                 while i < np.shape(wrong_captions_images)[0]:\n",
    "#                     if wrong_captions_images[i][1] == w_entry[1]:\n",
    "#                         _random_images = np.random.choice(n, 1, replace = False) #ERROR mit der 1\n",
    "#                         wrong_captions_images = np.array(self.data)[_random_images, :]\n",
    "\n",
    "#                     else:\n",
    "#                         random_index = np.random.choice(len(wrong_captions_images[i][0]), 1, replace = False)\n",
    "#                         caption = wrong_captions_images[i][0][random_index]\n",
    "#                         wrong_captions.append(caption[0])\n",
    "#                         i += 1\n",
    "                \n",
    "#                 w_entry[0] = wrong_captions\n",
    "                \n",
    "#                 _tmpw = self.load_image(str(w_entry[1].decode(\"utf-8\")))\n",
    "#                 w_entry[1] = _tmpw  \n",
    "                \n",
    "#                 ## Fill correct batch\n",
    "#                 # get 5 randomly selected captions for each image(out of 10 correct ones)\n",
    "#                 indecies_random_captions = np.random.choice(len(entry[0]), 1, replace = False)\n",
    "#                 entry[0] = entry[0][indecies_random_captions]\n",
    "                      \n",
    "#                 #load the actual image from the file\n",
    "#                 _tmp = self.load_image(str(entry[1].decode(\"utf-8\")))\n",
    "#                 entry[1] = _tmp\n",
    "            \n",
    "#             #create random embeddings for the generator\n",
    "#             # shuffle the data to get random batches\n",
    "#             random_indices_embeddings = np.random.choice(n, n, replace = False)\n",
    "#             data_pool_embeddings = np.array(self.data)[random_indices, :]\n",
    "            \n",
    "#             ## create the elements that are returned\n",
    "#             random_embeddings = []\n",
    "#             for embeddings in data_pool_embeddings[on:off][:,0]:\n",
    "#                 idx = np.random.choice(np.shape(embeddings)[0], 1)\n",
    "#                 random_embeddings.append(embeddings[idx][0])\n",
    "            \n",
    "#             real_embeddings = np.array([i[0] for i in batch[:,0]])\n",
    "#             real_images = np.array([i for i in batch[:,1]])\n",
    "\n",
    "#             wrong_embeddings = np.array([i[0] for i in wrong_batch[:,0]])   \n",
    "#             wrong_images = np.array([i for i in wrong_batch[:,1]])\n",
    "            \n",
    "#             yield real_embeddings, real_images, wrong_embeddings, wrong_images, random_embeddings\n",
    "  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    #image size is the desired dimension of all images that are in the batch. The are downsampled by the data loader.\n",
    "    #load_into_memory specifies whether the downsampled images should be laoded into memory. This can speed up\n",
    "    #                 training significantly. But of course it's not possbile to very large datasets\n",
    "    def __init__(self, directory, image_size=64, load_into_memory=False):\n",
    "        print(\"## BUILD DataLoader ##\")\n",
    "        self.load_into_memory = load_into_memory\n",
    "        self.directory = directory\n",
    "        self.image_size = image_size\n",
    "        self.data = self.load_binaries()\n",
    "        print(np.shape(self.data))\n",
    "        print(\"Nr. Trainingsamples: \" + str(np.shape(self.data)[0]))\n",
    "\n",
    "    def load_binaries(self):\n",
    "        try:\n",
    "            print(\"[Info] loading \" + self.directory + \"/\" + Settings.data_set)\n",
    "            data = np.load(self.directory + \"/\" + Settings.data_set)\n",
    "            if self.load_into_memory:\n",
    "                print(\"[Attention] Loading \"+str(np.shape(data)[0])+\" images with dimensions \"+str(self.image_size)+\"x\"+str(self.image_size)+\"x3 into memory!!\")\n",
    "                print(\"!!!!!  [TODO]   !!!!! progress bar\")\n",
    "                for row in data:\n",
    "                    img_file = row[1]\n",
    "                    img_matrix = self.load_image(str(img_file))\n",
    "                    row[2] = img_matrix\n",
    "            return data\n",
    "        except FileNotFoundError:\n",
    "            print(\"[Error] \"+ self.directory + \"/\" + Settings.data_set + \" not found!\")\n",
    "            sys.exit()\n",
    "\n",
    "        data = []\n",
    "        with open(self.directory + '/flowers_icml/trainvalclasses.txt') as file:\n",
    "            train_classes = file.readlines()\n",
    "        train_classes = [x.strip() for x in train_classes]\n",
    "\n",
    "        # go through all training classes to extract the captions\n",
    "        for c in train_classes:\n",
    "            #create a list of all files within the folder corresponding to the class/category\n",
    "            filelist = os.listdir(self.directory + '/flowers_icml/' + c)\n",
    "            #loop through all the files\n",
    "            for file in filelist:\n",
    "                #check if it's actually a t7 file. Only these contain the information about the images\n",
    "                if file.endswith(\".t7\"):\n",
    "                    # load the torchfile\n",
    "                    _tmp = torchfile.load(self.directory + '/flowers_icml/' + c + \"/\" + file)\n",
    "                    caption = _tmp.txt\n",
    "                    img_file = _tmp.img\n",
    "                    # add the caption the filename of the image to the loaded data\n",
    "                    img_matrix = self.load_image(str(img_file))\n",
    "                    data.append([caption, img_file, img_matrix])\n",
    "\n",
    "\n",
    "        print(np.shape(data))\n",
    "        print(\"return binaries\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def load_image(self, image_file):\n",
    "        img = skimage.io.imread(self.directory + \"/\" + image_file)\n",
    "        # GRAYSCALE\n",
    "        if len(img.shape) == 2:\n",
    "            print(\"GrayScale Image -- \" + image_file + \"\\n\")\n",
    "            img_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n",
    "            img_new[:,:,0] = img\n",
    "            img_new[:,:,1] = img\n",
    "            img_new[:,:,2] = img\n",
    "            img = img_new\n",
    "\n",
    "        img = skimage.transform.resize(img, (self.image_size, self.image_size))\n",
    "        return img\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        #total number of samples\n",
    "        n = np.shape(self.data)[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = n\n",
    "\n",
    "        # shuffle the data to get random batches\n",
    "        random_indeces = np.random.choice(n, n, replace = False)\n",
    "        data_pool = np.array(self.data)[random_indeces, :]\n",
    "\n",
    "        wrong_random_indices = np.random.choice(n, n, replace = False)\n",
    "        wrong_data_pool = np.array(self.data)[wrong_random_indices, :]\n",
    "\n",
    "        #create the batches\n",
    "        for i in range(n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "\n",
    "            wrong_batch = wrong_data_pool[on:off]\n",
    "            batch = data_pool[on:off]\n",
    "\n",
    "            for w_entry, entry in zip(wrong_batch, batch):\n",
    "\n",
    "                if not self.load_into_memory:\n",
    "                    _tmpw = self.load_image(str(w_entry[1]))\n",
    "                    w_entry[1] = _tmpw\n",
    "\n",
    "                    _tmp = self.load_image(str(entry[1]))\n",
    "                    entry[1] = _tmp\n",
    "                    ## Fill correct batch\n",
    "\n",
    "                # get 5 randomly selected captions for each image(out of 10 correct ones)\n",
    "                indecies_random_captions = np.random.choice(min(len(entry[0]), 5), 1, replace = False)\n",
    "                entry[0] = entry[0][indecies_random_captions]\n",
    "                entry[3] = entry[3][indecies_random_captions[0]]\n",
    "\n",
    "            real_embeddings = np.array([i[0] for i in batch[:,0]])\n",
    "            text_captions = [i for i in batch[:,3]]\n",
    "\n",
    "            real_images = np.array([i for i in batch[:,2]])\n",
    "            wrong_images = np.array([i for i in wrong_batch[:,2]])\n",
    "\n",
    "            yield real_embeddings, real_images, wrong_images, text_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# t_test = time.time()\n",
    "\n",
    "\n",
    "# data_loader = DataLoader(\"data\", load_into_memory=True)\n",
    "# t_test = time.time() - t_test\n",
    "# minutes_t, seconds_t = divmod(t_test, 60)\n",
    "# print((\"-- Loaded Binaries in {0: .0f}m{1: .2f}s.\").format( minutes_t,seconds_t))\n",
    "# t_test = time.time()\n",
    "# idx = 1\n",
    "\n",
    "# t_total = time.time()\n",
    "\n",
    "# t_test = time.time()\n",
    "# for real_embeddings, real_images, wrong_images, captions in data_loader.get_batches(64):\n",
    "#     t_test = time.time() - t_test\n",
    "#     minutes_t, seconds_t = divmod(t_test, 60)\n",
    "#     print((\"-- Finished Batch #{0} in {1: .0f}m{2: .2f}s.\").format(idx +1, minutes_t,seconds_t))\n",
    "#     t_test = time.time()\n",
    "#     idx += 1\n",
    "\n",
    "    \n",
    "# t_total = time.time() - t_total\n",
    "# minutes_t, seconds_t = divmod(t_total, 60)\n",
    "   \n",
    "# print((\"-- Finished Epoch in {0: .0f}m{1: .2f}s.\").format(minutes_t,seconds_t))\n",
    "\n",
    "# t_test = time.time()\n",
    "# for real_embeddings, real_images, wrong_images, captions in data_loader.get_batches(64):\n",
    "#     t_test = time.time() - t_test\n",
    "#     minutes_t, seconds_t = divmod(t_test, 60)\n",
    "#     print((\"-- Finished Batch #{0} in {1: .0f}m{2: .2f}s.\").format(idx +1, minutes_t,seconds_t))\n",
    "#     t_test = time.time()\n",
    "#     idx += 1\n",
    "\n",
    "    \n",
    "# t_total = time.time() - t_total\n",
    "# minutes_t, seconds_t = divmod(t_total, 60)\n",
    "   \n",
    "# print((\"-- Finished Epoch in {0: .0f}m{1: .2f}s.\").format(minutes_t,seconds_t))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################################################\n",
    "##### Generative Adversarial Network  ##########################\n",
    "################################################################\n",
    "\n",
    "class GAN(object):\n",
    "\n",
    "    def __init__(self, start_run=0):\n",
    "        print(\"\\n###################################\")\n",
    "        print(\"#           Build Network          #\")\n",
    "        print(\"###################################\\n\")\n",
    "        \n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default(): \n",
    "            self.build_graph()\n",
    "\n",
    "        self.run = start_run\n",
    "    \n",
    "\n",
    "    def build_graph(self):\n",
    "        self.ist_training = tf.placeholder(tf.bool, None)\n",
    "        \n",
    "        #########Embedding Placeholders########\n",
    "        self.text_embeddings = tf.placeholder(tf.float32,shape=[None,1024], name=\"text_embeddings\")\n",
    "        self.text_embeddings_real = tf.placeholder(tf.float32,shape=[None,1024], name=\"real_text_embeddings\")\n",
    "        self.text_embeddings_wrong = tf.placeholder(tf.float32,shape=[None,1024], name=\"wrong_text_embeddings\")\n",
    "        \n",
    "        ###### Build Generator####\n",
    "        self.build_generator()\n",
    "        \n",
    "        ####### Placeholders for Images####\n",
    "        with tf.variable_scope(\"real_images\"):\n",
    "            self.real_images = batch_norm(tf.placeholder(tf.float32, shape=[Settings.batch_size,64, 64, 3], name=\"real_images\"),[0], self.ist_training)\n",
    "            #self.real_images = tf.reshape(tf.placeholder(tf.float32, shape=[Settings.batch_size, 64, 64, 3],[Settings.batch_size,\n",
    "                                                                                                                #64,64,3]))\n",
    "            \n",
    "        with tf.variable_scope(\"wrong_images\"):\n",
    "            self.wrong_images = batch_norm(tf.placeholder(tf.float32,shape=[Settings.batch_size,64, 64, 3], name=\"wrong_images\"), [0], self.ist_training)\n",
    "            #self.real_images_wrong_captions = tf.reshape(tf.placeholder(tf.float32,shape=[Settings.batch_size,1, 64, 64, 3], [Settings.batch_size,\n",
    "                                                                                                                          #64,64,3]))\n",
    "            \n",
    "        #Batch Normalize the real images moeglicher FEHLER\n",
    "        \n",
    "        #print(\"self.real_images: \" + str(np.shape(self.real_images)))\n",
    "        #print(\"self.real_images_wrong_captions: \" + str(np.shape(self.real_images_wrong_captions)))\n",
    "\n",
    "        ######CONCATINATE IMAGES#########\n",
    "#         concat_images = tf.concat([self.real_images, self.wrong_images], axis=0)\n",
    "#         #print(\"self.generated_images: \" + str(np.shape(self.generated_images)))\n",
    "        \n",
    "#         concat_images = tf.concat([self.generated_images, concat_images], axis=0)\n",
    "#         #[real_images, real_images_wrong_captions, generated_images]\n",
    "#         #print(\"concat_images: \" + str(np.shape(concat_images)))\n",
    "        \n",
    "#         ########CONCATINATE EMBEDDINGS########\n",
    "#         concat_embeddings = tf.concat([self.text_embeddings_real, self.text_embeddings_wrong], axis=0)\n",
    "#         concat_embeddings = tf.concat([concat_embeddings, self.text_embeddings], axis=0)\n",
    "        \n",
    "#         ###Build Discriminator######\n",
    "# #         self.build_discriminator(concat_images, concat_embeddings)\n",
    "        \n",
    "        dis_real_logits, dis_real_sigmoid   = self.discriminator(self.real_images, self.text_embeddings_real)\n",
    "        dis_wrong_logits, dis_wrong_sigmoid = self.discriminator(self.wrong_images, self.text_embeddings_wrong, reuse = True)\n",
    "        dis_fake_logits, dis_fake_image     = self.discriminator(self.generated_images, self.text_embeddings, reuse = True)\n",
    "               \n",
    "        ######Build Optimizer######\n",
    "        gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_fake_logits, labels=tf.ones_like(dis_fake_logits)))\n",
    "        \n",
    "        dis_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_real_logits, labels=tf.ones_like(dis_real_logits)))\n",
    "        dis_loss_wrong = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_wrong_logits, labels=tf.zeros_like(dis_wrong_logits)))\n",
    "        dis_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_fake_logits, labels=tf.zeros_like(dis_fake_logits)))\n",
    "        \n",
    "        dis_loss = dis_loss_real + ((dis_loss_wrong + dis_loss_fake) * 0.5)\n",
    "        \n",
    "        ####OTHER####\n",
    "        #real_score  = tf.reduce_mean(dis_real_sigmoid)\n",
    "        #wrong_score = tf.reduce_mean(dis_wrong_sigmoid)\n",
    "        #fake_score  = tf.reduce_mean(dis_fake_image )\n",
    "        #dis_loss = -((tf.log(real_score) + (tf.log(1 - wrong_score) + tf.log(1 - fake_score)) / 2))\n",
    "        #gen_loss = -tf.log(fake_score)\n",
    "  \n",
    "        self.dis_summary = tf.summary.scalar('Discriminator_Loss',dis_loss)\n",
    "        self.gen_summary = tf.summary.scalar('Generator_Loss',gen_loss)\n",
    "        \n",
    "        ##########################variable lists#########################################\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        dis_variables = [var for var in trainable_variables if \"dis\" in var.name]\n",
    "        gen_variables = [var for var in trainable_variables if \"gen\" in var.name]\n",
    "\n",
    "        \n",
    "        #############################Optimizer#############\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            self.dis_optimizer = tf.train.AdamOptimizer(Settings.lr, beta1=Settings.momentum).minimize(dis_loss, var_list=dis_variables)\n",
    "            self.gen_optimizer = tf.train.AdamOptimizer(Settings.lr, beta1=Settings.momentum).minimize(gen_loss, var_list=gen_variables)\n",
    "\n",
    "        \n",
    "#         self.optimizer()\n",
    "\n",
    "        \n",
    "    ###################################################\n",
    "    ######## BUILD GENERATOR##########################\n",
    "    def build_generator(self):\n",
    "      #tf.reset_default_graph()\n",
    "        \n",
    "       ######## Inputs ##########\n",
    "        self.z_vector = tf.placeholder(tf.float32, shape=[None,100], name=\"z\")\n",
    "        #self.is_training=tf.placeholder(tf.bool)\n",
    "        with tf.variable_scope(\"gen_reduce_embeddings\"):\n",
    "            #####norm here#####\n",
    "            reduced_embeddings=feedforward_layer(self.text_embeddings,[1024,128],activation=lrelu)\n",
    "            concat_input=tf.concat([self.z_vector,reduced_embeddings],1)\n",
    "        \n",
    "        \n",
    "        #####layer1######\n",
    "        with tf.variable_scope(\"gen_first_layer\"):\n",
    "            drive_first=feedforward_layer(concat_input,[228,4*4*1024])\n",
    "            reshaped_first=tf.reshape(drive_first,[Settings.batch_size,4,4,1024])\n",
    "            output_first=tf.nn.relu(batch_norm(reshaped_first,[0,1,2], self.ist_training))\n",
    "            #print(np.shape(output_first))\n",
    "       \n",
    "        ####layer2####\n",
    "        with tf.variable_scope(\"gen_second_layer\"):\n",
    "            #welche kernel größe??\n",
    "            #deconv_layer(input, target_shape, filter, strides, padding, bias_init, norm_axes=[0,1,2], normalize=False, activation=None)\n",
    "            d_second_layer = deconv_layer(output_first,[Settings.batch_size,8,8,512],[5,5,512,1024],[1,2,2,1],\"SAME\",normalize=True,activation=tf.nn.relu)\n",
    "            second_layer=tf.nn.dropout(d_second_layer,Settings.keep_prop)\n",
    "            #print(np.shape(second_layer))\n",
    "        \n",
    "        #####layer3#####\n",
    "        with tf.variable_scope(\"gen_third_layer\"):\n",
    "            third_layer = deconv_layer(second_layer,[Settings.batch_size,16,16,256],[5,5,256,512], [1,2,2,1],\"SAME\", normalize=True, activation=tf.nn.relu)\n",
    "            #print(np.shape(third_layer))     \n",
    "        \n",
    "        ######layer4######\n",
    "        with tf.variable_scope(\"gen_fourth_layer\"):\n",
    "            d_fourth_layer=deconv_layer(third_layer,[Settings.batch_size,32,32,128],[5,5,128,256], [1,2,2,1],\"SAME\", normalize=True, activation=tf.nn.relu)\n",
    "            fourth_layer=tf.nn.dropout(d_fourth_layer,Settings.keep_prop)\n",
    "            #print(np.shape(fourth_layer))\n",
    "        \n",
    "        #####layer5#####\n",
    "        with tf.variable_scope(\"gen_fifth_layer\"):\n",
    "            fifth_layer=deconv_layer(fourth_layer,[Settings.batch_size,64,64,3],[5,5,3,128], [1,2,2,1],\"SAME\",normalize=True, activation=tf.nn.tanh)\n",
    "            ###VORSICHT wir haben es zur tanh geaendert\n",
    "            #print(np.shape(fith_layer))\n",
    "        self.generated_images = (fifth_layer + 1) * 0.5\n",
    "        #return self.generated_images\n",
    "                                \n",
    "\n",
    "    def discriminator(self, discriminator_images, text_embeddings, reuse=False):\n",
    "#         if reuse:\n",
    "#             tf.get_variable_scope().reuse_variables()\n",
    "        \n",
    "        ############inputs############\n",
    "        with tf.variable_scope(\"dis_batch_norm\", reuse=reuse):\n",
    "            input_images = batch_norm(discriminator_images, [0,1,2], self.ist_training)\n",
    "        ## VORSICHT BATCHNORM HINZUGEFUEGT\n",
    "        #print(\"input_images: \" + str(np.shape(input_images)))\n",
    "#         is_training=tf.placeholder(tf.bool)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"dis_first_layer\", reuse=reuse):\n",
    "            #64x64x3\n",
    "            state_1 = conv2d_layer(input_images, filter=[5,5,3,64], strides=[1,2,2,1], padding=\"SAME\", normalize=True, activation=lrelu)\n",
    "            #print(\"state_1: \" + str(np.shape(state_1)))\n",
    "        with tf.variable_scope(\"dis_second_layer\", reuse=reuse):\n",
    "            #32x32x128\n",
    "            state_2 = conv2d_layer(state_1, filter=[5,5,64,32], strides=[1,2,2,1], padding=\"SAME\", normalize=True, activation=lrelu)\n",
    "            #print(\"state_2: \" + str(np.shape(state_2)))\n",
    "        with tf.variable_scope(\"dis_third_layer\", reuse=reuse):\n",
    "            #16x16x256\n",
    "            state_3 = conv2d_layer(state_2, filter=[5,5,32,16], strides=[1,2,2,1], padding=\"SAME\", normalize=True, activation=lrelu)\n",
    "            #print(\"state_3: \" + str(np.shape(state_3)))\n",
    "        with tf.variable_scope(\"dis_fourth_layer\", reuse=reuse):\n",
    "            #8x8x512\n",
    "            state_4 = conv2d_layer(state_3, filter=[5,5,16,4], strides=[1,2,2,1], padding=\"SAME\", normalize=True, activation=lrelu)\n",
    "            #out: 4x4x1024\n",
    "            #print(\"state_4: \" + str(np.shape(state_4)))\n",
    "\n",
    "        ####  ADD TEXT EMBEDDINGS TO NETWORK  ####\n",
    "        with tf.variable_scope(\"dis_reduce_embeddings\", reuse=reuse):\n",
    "            reduced_embeddings = feedforward_layer(text_embeddings, [1024,128], activation=tf.nn.relu)\n",
    "            reduced_embeddings = tf.expand_dims(reduced_embeddings,1)\n",
    "            reduced_embeddings = tf.expand_dims(reduced_embeddings,2)\n",
    "            tiled_embeddings = tf.tile(reduced_embeddings, [1,4,4,1], name='tile_embeddings')\n",
    "        \n",
    "        ### CONCAT TEXT EMBEDDINGS AND STATE_4  ###\n",
    "        \n",
    "        with tf.variable_scope(\"dis_concat_layer\", reuse=reuse):\n",
    "            state_4_concat = tf.concat([state_4, tiled_embeddings], 3)\n",
    "            \n",
    "        with tf.variable_scope(\"dis_fifth_layer\", reuse=reuse):\n",
    "            state_5 = conv2d_layer(state_4_concat, filter=[1,1,132,4], strides=[1,1,1,1], padding=\"SAME\", normalize=True, activation=tf.nn.relu)\n",
    "            #out: 4x4x132 \n",
    "            print(np.shape(state_5))\n",
    "            print(\"---------=\")\n",
    "    ### hier normalizieren?? oder danach\n",
    "            state5_flat = tf.reshape(state_5, [Settings.batch_size, -1])\n",
    "            print(np.shape(state5_flat))\n",
    "    ####oder hier??\n",
    "        with tf.variable_scope(\"dis_sixth_layer\", reuse=reuse):\n",
    "            #TODO PUT ACTIVATION FUNTION BACK\n",
    "            state_6 = feedforward_layer(state5_flat, [64, 1], norm_axes=[0], normalize=False, activation=None)\n",
    "        \n",
    "        self.dis_out = state_6\n",
    "        print(\"dis_out: \" + str(np.shape(self.dis_out)))\n",
    "        \n",
    "        return state_6, tf.nn.sigmoid(state_6)\n",
    "    \n",
    "    ################################################################\n",
    "    #########  NETWORK FUNCTIONS  ##################################\n",
    "    ################################################################\n",
    "    def optimizer(self):\n",
    "        ### output from the generator\n",
    "        #[real_images, real_images_wrong_captions, generated_images]\n",
    "        \n",
    "        #########################Loss###################################################\n",
    "#         ones        = tf.ones(shape=[Settings.batch_size])\n",
    "#         real_score  = tf.reduce_mean(self.dis_out[0:Settings.batch_size])\n",
    "#         wrong_score = tf.reduce_mean(self.dis_out[Settings.batch_size:Settings.batch_size * 2])\n",
    "#         fake_score  = tf.reduce_mean(self.dis_out[Settings.batch_size * 2:Settings.batch_size * 3])\n",
    "        \n",
    "#         dis_loss = tf.log(real_score) + (tf.log(1 - wrong_score) + tf.log(1 - fake_score)) / 2\n",
    "#         gen_loss = tf.log(fake_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        real_score  = self.dis_out[0:Settings.batch_size]\n",
    "        wrong_score = self.dis_out[Settings.batch_size:Settings.batch_size * 2]\n",
    "        fake_score  = self.dis_out[Settings.batch_size * 2:Settings.batch_size * 3]\n",
    "      \n",
    "        gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_score, labels=tf.ones_like(fake_score)))\n",
    "        \n",
    "        dis_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_score, labels=tf.ones_like(real_score)))\n",
    "        dis_loss_wrong = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=wrong_score, labels=tf.zeros_like(wrong_score)))\n",
    "        dis_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_score, labels=tf.zeros_like(fake_score)))\n",
    "        \n",
    "        dis_loss = dis_loss_real + (dis_loss_wrong + dis_loss_fake) * 0.5\n",
    "        \n",
    "        self.dis_summary = tf.summary.scalar('Discriminator_Loss',dis_loss)\n",
    "        self.gen_summary = tf.summary.scalar('Generator_Loss',gen_loss)\n",
    "        \n",
    "        ##########################variable lists#########################################\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        dis_variables = [var for var in trainable_variables if \"dis\" in var.name]\n",
    "        gen_variables = [var for var in trainable_variables if \"gen\" in var.name]\n",
    "\n",
    "        \n",
    "        #############################Optimizer#############\n",
    "        \n",
    "        self.dis_optimizer = tf.train.AdamOptimizer(Settings.lr, beta1=Settings.momentum).minimize(dis_loss, var_list=dis_variables)\n",
    "        self.gen_optimizer = tf.train.AdamOptimizer(Settings.lr, beta1=Settings.momentum).minimize(gen_loss, var_list=gen_variables)\n",
    "        \n",
    "        #################noch die Nodes für Tensorboard schreiben######################\n",
    "        print('PferdePferdePferde')\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "def conv2d_layer(input, filter, strides, padding, bias_init=0.0, norm_axes=[0,1,2], normalize=False, activation=None):\n",
    "    depth = input.shape[-1]\n",
    "    fan_in = int(input.shape[1] * input.shape[2])\n",
    "    \n",
    "    if activation == tf.nn.relu or activation== lrelu:\n",
    "        var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "    else:\n",
    "        var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "    W = tf.get_variable('weights', filter, initializer=var_init)\n",
    "    #variable summaries\n",
    "    b = tf.get_variable('biases', filter[-1], initializer=tf.constant_initializer(bias_init))\n",
    "    #variable summaries\n",
    "\n",
    "    state = tf.nn.conv2d(input,W, strides, padding) + b\n",
    "    #state_depth=state.shape[-1]\n",
    "\n",
    "    if normalize:\n",
    "        state=batch_norm(state,norm_axes, is_training)\n",
    "         \n",
    "    conv_out = state\n",
    "\n",
    "    if not(activation is None):\n",
    "        conv_out = activation(state)\n",
    "    \n",
    "    return conv_out\n",
    "\n",
    "def deconv_layer(input, target_shape, filter, strides, padding, bias_init=0.0, norm_axes=[0,1,2], normalize=False, activation=None, is_training=None):\n",
    "    depth = input.shape[-1]\n",
    "    fan_in = int(input.shape[1] * input.shape[2])\n",
    "    \n",
    "    if activation == tf.nn.relu or activation == lrelu:\n",
    "        var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "    else:\n",
    "        var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "    \n",
    "    W = tf.get_variable('weights', [filter[0], filter[1], target_shape[-1], depth], initializer=var_init)\n",
    "    #variable summaries\n",
    "    b = tf.get_variable('biases', target_shape[-1], initializer=tf.constant_initializer(bias_init))\n",
    "    #variable summaries\n",
    "\n",
    "    print(str(np.shape(input)) + \"input shape\")\n",
    "    state = tf.nn.conv2d_transpose(input, W, target_shape, strides, padding) + b\n",
    "    #state_depth=state.shape[-1]\n",
    "\n",
    "    if normalize:\n",
    "        state=batch_norm(state,norm_axes, is_training)\n",
    "         \n",
    "    conv_out = state\n",
    "\n",
    "    if not(activation is None):\n",
    "        conv_out = activation(state)\n",
    "    \n",
    "    return conv_out\n",
    "\n",
    "def feedforward_layer(input, weights, bias_init=0.0, norm_axes=[0], normalize=False, activation=None, is_training=None):\n",
    "    depth= input.shape[-1]\n",
    "    fan_in = int(input.shape[-1])\n",
    "    \n",
    "    if activation == tf.nn.relu or activation == lrelu:\n",
    "        var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "    else:\n",
    "        var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "    \n",
    "    W = tf.get_variable('weights', weights, tf.float32,var_init)\n",
    "    #variable summaries\n",
    "    b = tf.get_variable('biases', weights[-1], initializer=tf.constant_initializer(bias_init))\n",
    "    #variable summaries\n",
    "\n",
    "    state = tf.matmul(input,W) + b\n",
    "    #state_depth=state.shape[-1]\n",
    "\n",
    "    if normalize:\n",
    "        state = batch_norm(state,norm_axes, is_training)\n",
    "          \n",
    "    ff_out = state\n",
    "\n",
    "    if not(activation is None):\n",
    "        ff_out = activation(state)\n",
    "\n",
    "    return ff_out\n",
    "\n",
    "def flatten(x):\n",
    "    size = int(np.prod(x.shape[1:]))\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n",
    "# def batch_norm(inp,norm_axes):\n",
    "#     depth = inp.shape[-1]\n",
    "#     epsilon = 1e-6\n",
    "#     mean, var = tf.nn.moments(inp, norm_axes)\n",
    "#     offset = tf.get_variable('offset1', depth, dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "#     scale = tf.get_variable('scale1', depth, dtype=tf.float32, initializer=tf.constant_initializer(1.0))\n",
    "#     state = tf.nn.batch_normalization(inp, mean, var, offset, scale, epsilon)\n",
    "#     return state\n",
    "def _pop_batch_norm(x, pop_mean, pop_var, offset, scale):\n",
    "            return tf.nn.batch_normalization(x, pop_mean, pop_var, offset, scale, 1e-6)\n",
    "\n",
    "def _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale):\n",
    "    decay = 0.99\n",
    "\n",
    "    dependency_1 = tf.assign(pop_mean, pop_mean * decay + mean * (1 - decay))\n",
    "    dependency_2 = tf.assign(pop_var, pop_var * decay + var * (1 - decay))\n",
    "\n",
    "    with tf.control_dependencies([dependency_1, dependency_2]):\n",
    "    return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)\n",
    "\n",
    "def batch_norm(x, axes, is_training):\n",
    "    depth = x.shape[-1]\n",
    "    mean, var = tf.nn.moments(x, axes = axes)\n",
    "\n",
    "    var_init = tf.constant_initializer(0.0)\n",
    "    offset = tf.get_variable(\"offset\", depth, tf.float32, var_init)\n",
    "    var_init = tf.constant_initializer(1.0)\n",
    "    scale = tf.get_variable(\"scale\", depth, tf.float32, var_init)\n",
    "\n",
    "    pop_mean = tf.get_variable(\"pop_mean\", depth, initializer = tf.zeros_initializer(), trainable = False)\n",
    "    pop_var = tf.get_variable(\"pop_var\", depth, initializer = tf.ones_initializer(), trainable = False)\n",
    "    \n",
    "    return _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale)\n",
    "#     return tf.cond(\n",
    "#         is_training,\n",
    "#         lambda: _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale),\n",
    "#         lambda: _pop_batch_norm(x, pop_mean, pop_var, offset, scale)\n",
    "#     )\n",
    "\n",
    "def lrelu(x, alpha=0.2):\n",
    "    return tf.nn.relu(x) - alpha * tf.nn.relu(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#################################################################################\n",
    "###############################TRAINING###########################################\n",
    "###################################################################################\n",
    "# net: Network\n",
    "# boolean restore: restores from last checkpoint last run \n",
    "\n",
    "def train(net,gen,epochs,restore):\n",
    "    ######create folder to store generated images#######\n",
    "    if not os.path.exists(\"./generated_pictures/\"+str(Settings.run)):\n",
    "        os.makedirs(\"./generated_pictures/\"+str(Settings.run))\n",
    "    ##saver=tf.train.Saver()\n",
    "    #saver.restore(session, \"./weights/.data-00000-of-00001\")-> latest checkpoint\n",
    "    \n",
    "    ######### create Write for Tensorboard################\n",
    "    discrimiator_writer=tf.summary.FileWriter('./summary_test/'+str(Settings.run)+'/discriminator',tf.get_default_graph())\n",
    "    generator_writer=tf.summary.FileWriter('./summary_test/'+str(Settings.run)+'/generator')\n",
    "    \n",
    "    print(\"\\n###################################\")\n",
    "    print(\"#          Start Training         #\")\n",
    "    print(\"###################################\\n\")\n",
    "    \n",
    "    ########################SESSION############################\n",
    "    with tf.Session(graph=net.graph) as session:\n",
    "        #session.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        last_real_images=np.zeros([Settings.batch_size,64,64,3])\n",
    "        to_visualize=np.ndarray(shape=(42,64,64,3))\n",
    "        ############RESTORE PARAMETERS IF WANTED#########################\n",
    "        if restore:\n",
    "            #new_saver = tf.train.import_meta_graph('./stored-1.meta')\n",
    "            #new_saver=tf.train.import_meta_graph(\"./store_weights/\"+str(Settings.run-1)+\"/stored.meta\")\n",
    "            saver.restore(session, tf.train.latest_checkpoint('./store_weights/'+str(Settings.run-1)))\n",
    "            #saver.restore(session, tf.train.latest_checkpoint('./store_weights/'+str(507)))\n",
    "            #saver.restore(session, \"./horse/stored.meta\")\n",
    "        else:\n",
    "            session.run(tf.global_variables_initializer())\n",
    "        #############Training#################################################################################################\n",
    "        step=0\n",
    "        for epoch in range(epochs):\n",
    "            t = time.time()\n",
    "            ###############das ist nur RANDOM data zum TESTEN##################\n",
    "            #z=np.random.uniform(-1,1,[32,100])\n",
    "            #eb=np.random.uniform(-1,1,[32,1024])\n",
    "            #with open('generated.pickle', 'rb') as f:\n",
    "            #    restored=pickle.load(f)\n",
    "        #############Training#################################################################################################\n",
    "            #for real_embeddings, real_images, wrong_images in gen.get_batches(Settings.batch_size):\n",
    "            for real_embeddings, real_images, wrong_images, real_captions in gen.get_batches(Settings.batch_size):\n",
    "#                 print(\"real_embeddings: \" + str(np.shape(real_embeddings)))\n",
    "#                 print(\"real_images: \" + str(np.shape(real_images)))\n",
    "#                 print(\"wrong_embeddings: \" + str(np.shape(wrong_embeddings)))\n",
    "#                 print(\"wrong_images: \" + str(np.shape(wrong_images)))\n",
    "#                 print(\"random_embeddings: \" + str(np.shape(random_embeddings)))\n",
    "            #batch,wrong_batch=next(gen_test)\n",
    "                #z = np.random.uniform(-1,1,[Settings.batch_size,100])\n",
    "                z = np.random.normal(0 , 1 ,[ Settings.batch_size , 100 ] )\n",
    "        \n",
    "                #real_images=batch[:,1]\n",
    "                #wrong_images=wrong_batch[:,1]\n",
    "#                 with open('generated.pickle', 'rb') as f:\n",
    "#                     restored=pickle.load(f)\n",
    "#                 real_images=restored\n",
    "#                 wrong_images=restored\n",
    "#                 real_eb=np.random.uniform(-1,1,[Settings.batch_size,1024])\n",
    "#                 wrong_eb=np.random.uniform(-1,1,[Settings.batch_size,1024])\n",
    "                #real_eb = [emb[0] for emb in batch[:,0]]\n",
    "                #wrong_eb = [emb[0] for emb in wrong_batch[:,0]]\n",
    "                \n",
    "                #real_eb=batch[:,0]\n",
    "                #wrong_eb=batch[:,0]\n",
    "            #a=batch[0:32]\n",
    "################################need this to get rid of embediings!!!!!!!!!!!!!\n",
    "            #restored=np.delete(a, 0, axis=1)\n",
    "            #print('#########################')\n",
    "            #print(np.shape(restored))\n",
    "            #möchte liste von Bildern bekommen und rein bilder und reihn text embedings\n",
    "            #is training noch machen\n",
    "            #weights abspeichern\n",
    "                feed_dict = {net.z_vector: z,\n",
    "                             net.real_images: real_images,\n",
    "                             net.text_embeddings_real: real_embeddings,\n",
    "                             net.wrong_images: wrong_images,\n",
    "                             net.text_embeddings_wrong: real_embeddings,\n",
    "                             net.text_embeddings: real_embeddings,\n",
    "                             net.ist_training: True}\n",
    "    \n",
    "                generated_images, _, _, dis_sum, gen_sum = session.run([net.generated_images,net.dis_optimizer,net.gen_optimizer,net.dis_summary,net.gen_summary],\n",
    "                            feed_dict=feed_dict)\n",
    "\n",
    "                ##############Tensorboard summaries###############\n",
    "                discrimiator_writer.add_summary(dis_sum,step)\n",
    "                generator_writer.add_summary(gen_sum,step)\n",
    "                step += 1\n",
    "                last_real_images=real_images\n",
    "\n",
    "            #####################Save Session###########################\n",
    "            #saver.save(session, \"./stored\",1)\n",
    "            saver.save(session,\"./store_weights/\"+str(Settings.run)+\"/stored\",step)\n",
    "            t = time.time() - t\n",
    "            minutes, seconds = divmod(t, 60)\n",
    "            print((\"-- Finished Epoch #{0} in {1: .0f}m{2: .2f}s.\").format(epoch +1, minutes,seconds))\n",
    "            \n",
    "            \n",
    "        \n",
    "        #train_saver.save(session, './weight_dir', global_step=step)\n",
    "        ######generated images als object abspeichern??\n",
    "        #with open('generated.pickle', 'wb') as f:\n",
    "        #    pickle.dump(generated_images, f)\n",
    "        \n",
    "        \n",
    "        ##########################Store Visualizations###############################\n",
    "            if Settings.show_captions:\n",
    "                fig = visual_with_captions(generated_images[0:8], real_captions[0:8])\n",
    "            else:\n",
    "                to_visualize[0:7]=generated_images[0:7]\n",
    "                to_visualize[7:14]=last_real_images[0:7]\n",
    "                to_visualize[14:21]=generated_images[7:14]\n",
    "                to_visualize[21:28]=last_real_images[7:14]\n",
    "                to_visualize[28:35]=generated_images[14:21]\n",
    "                to_visualize[35:42]=last_real_images[14:21]\n",
    "\n",
    "                fig=visual(to_visualize)\n",
    "            #fig = visual(generated_images[0:32])\n",
    "            plt.savefig(\"./generated_pictures/\"+str(Settings.run)+\"/\"+str(step)+\".png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "#################################################################################\n",
    "############################### Visualization##################################\n",
    "##############################################################################\n",
    "#### call with images as batches e.g fig=visual(batch[0:39])\n",
    "#### saving the figure with:\n",
    "### plt.savefig('first_try.svg')\n",
    "\n",
    "#%matplotlib inline\n",
    "def visual(images, colums=7):\n",
    "    \n",
    "    #fig.canvas.set_window_title(label)\n",
    "    generated=False\n",
    "    if(np.shape(images[0]) == (64,64,3)):\n",
    "        generated = True\n",
    "        \n",
    "    nr_images = len(images)-len(images)%colums\n",
    "    row = nr_images/colums\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    \n",
    "    for i in range(nr_images):\n",
    "        ax=fig.add_subplot(row,colums,i+1)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        #print(np.shape(generated_images[0][0]))\n",
    "        if generated:\n",
    "            imgplot=ax.imshow(images[i])\n",
    "        else:\n",
    "            imgplot=ax.imshow(images[i][1])\n",
    "            \n",
    "    return fig\n",
    "\n",
    "#### for the captions  hab ich noch nicht mit unseren captions probieren können.\n",
    "def visual_with_captions(images, captions):\n",
    "    \n",
    "    captions = insert_breaks(captions,23)\n",
    "    length = len(images)-len(images)%4\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    fig.canvas.set_window_title(\"\")\n",
    "    i = 0\n",
    "    count = 0\n",
    "    \n",
    "    while count < length:\n",
    "        ax = fig.add_subplot(length/2,4,i+1)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        imgplot = ax.imshow(images[count])\n",
    "        ax = fig.add_subplot(length/2,4,i+2,frameon=False)\n",
    "        ax.text(-0.15,0.6,captions[count])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        i = i+2\n",
    "        count = count+1\n",
    "    return fig\n",
    "##inserts line breaks into each caption, inserts break at the position of the next space after\n",
    "#### the parameter number.\n",
    "\n",
    "def insert_breaks(captions, number):\n",
    "    #transformed=captions\n",
    "    for index in range(len(captions)):\n",
    "#       rest = len(caption)\n",
    "#       while rest >= number:\n",
    "#            while len(caption) <= pos\n",
    "        lines = []\n",
    "        for i in range(0, len(captions[index]), number):\n",
    "                lines.append(captions[index][i:i+number])\n",
    "        captions[index] =  '\\n'.join(lines)\n",
    "\n",
    "\n",
    "#    for ind in range(len(captions)):\n",
    "#        i=1\n",
    "#        rest = len(captions[ind])\n",
    "#        print(\"Rest: \" + str(rest))\n",
    "#        pos = 0\n",
    "#        while rest>=number:\n",
    "##  or ((captions[ind][pos] != ' ') and (len(captions[ind]) <= pos)\n",
    "#            while ((pos % number !=  0)):\n",
    "#                print(\"len: \" + str(len(captions[ind])) + \" pos: \" + str(pos))\n",
    "#                pos = pos+1\n",
    "#            rest = len(captions[ind]) - 1 - pos\n",
    "#            captions[ind] = str(captions[ind][0:pos])+'\\n'+ str(captions[ind][pos+2:len(captions[ind])-1])\n",
    "#            print(str(captions[ind]))\n",
    "#            i=i+1\n",
    "    return captions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visual_with_captions(images, captions):\n",
    "    print('1')\n",
    "    captions = insert_breaks(captions,23)\n",
    "    print(6)\n",
    "    length = len(images)-len(images)%4\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    fig.canvas.set_window_title(\"\")\n",
    "    i = 0\n",
    "    count = 0\n",
    "    \n",
    "    while count < length:\n",
    "        ax = fig.add_subplot(length/2,4,i+1)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        imgplot = ax.imshow(images[count])\n",
    "        ax = fig.add_subplot(length/2,4,i+2,frameon=False)\n",
    "        ax.text(-0.15,0.6,captions[count])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        i = i+2\n",
    "        count = count+1\n",
    "    plt.savefig('first2_try.svg')\n",
    "    return fig\n",
    "def insert_breaks(captions, number):\n",
    "    \n",
    "    for index in range(len(captions)):\n",
    "    lines =[]\n",
    "        for i in range(0, len(captions[index]), number):\n",
    "                lines.append(captions[index][i:i+number])\n",
    "        captions[index] =  '\\n'.join(lines)\n",
    "    return captions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_breaks(captions,number):\n",
    "    #transformed=captions\n",
    "    print(2)\n",
    "    for ind in range(len(captions)):\n",
    "        i=1\n",
    "        rest=len(captions[ind])\n",
    "        print(3)\n",
    "        while rest>=number:\n",
    "            \n",
    "            print(4)\n",
    "            pos=number*i\n",
    "            while((captions[ind][pos]!= ' ')and(len(captions[ind])< pos)):\n",
    "                pos=pos+1\n",
    "            rest=len(captions[ind])-1-pos\n",
    "            captions[ind]=str(captions[ind][0:pos])+'\\n'+ str(captions[ind][pos:len(captions[ind])])\n",
    "            i=i+1\n",
    "            \n",
    "        print(5)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## BUILD DataLoader ##\n",
      "[Info] loading data/flower_embeddings_custom.npy\n",
      "(4355, 4)\n",
      "Nr. Trainingsamples: 4355\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader('data', load_into_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mia\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thin, almost transparent yellow petals surrounding a dark yellow pistil and green sepal.', 'the petals of this flower are pink with a short stigma', 'the bloom of the flower is white in color and has thin white filaments with yellow anthers.', 'the flower has pink petals that are rough on the edge and a pink and orange pistil.', 'this flower has petals that are red with white stigma', 'very broad meaty pedals in dark pink-red with small white spots form a firm foundation for a small cluster of yellow stamen around no discernible pistils.', 'flower has petals that are white with white stigma and green pedical.', 'the round flower features a yellow and black stamen.', 'this flower is white in color, with petals that are uneven on the edges.', 'the petals on this flower are yellow with yellow stamen.', 'small flower with ruffled red petals and multiple white stamen in the middle', 'this flower has small orange and red petals as well as white pistil as its main features', 'the petals on this flower are red with yellow stamen.', 'flower has several yellow petals with yellow stamen sticking out from middle', 'the flower has many shades of pink for its petals.']\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-6cba72249afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreal_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrong_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_captions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_captions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisual_with_captions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_captions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-599c0810b346>\u001b[0m in \u001b[0;36mvisual_with_captions\u001b[1;34m(images, captions)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mimgplot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframeon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1708\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1709\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1710\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1711\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5192\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5194\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5195\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    598\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[0;32m    599\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Image data cannot be converted to float\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         if not (self._A.ndim == 2\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data cannot be converted to float"
     ]
    }
   ],
   "source": [
    "real_embeddings, real_images, wrong_images, real_captions=next(dl.get_batches(15))\n",
    "print(real_captions)\n",
    "fig=visual_with_captions(real_images, real_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this flower is red and \n",
      "hite in color, with pe\n",
      "als that are ova shape\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-1d14909590d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(len(real_captions[0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(len(real_captions[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisual_with_captions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreals_captions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#plt.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-c6de8f003be0>\u001b[0m in \u001b[0;36mvisual_with_captions\u001b[1;34m(images, captions)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mimgplot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframeon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1708\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1709\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1710\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1711\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5192\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5194\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5195\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    598\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[0;32m    599\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Image data cannot be converted to float\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         if not (self._A.ndim == 2\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data cannot be converted to float"
     ]
    }
   ],
   "source": [
    "#print('lenght rela:'+str(len(real_captions[0])))\n",
    "print(real_captions[10])\n",
    "reals_captions=['the petals are yellow and squareish, centered around some long yellow stamen.', 'this flower is orange and white in color with petals that are ruffled in appearance.']\n",
    "#print(len(real_captions[0]))\n",
    "#print(len(real_captions[1]))\n",
    "fig=visual_with_captions(real_images, reals_captions)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the petals on this flow\n",
      "er are yellow with yel\n",
      "low stamen.\n"
     ]
    }
   ],
   "source": [
    "print(real_captions[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## BUILD DataLoader ##\n",
      "[Info] loading data/flower_embeddings_custom.npy\n",
      "[Attention] Loading 4355 images with dimensions 64x64x3 into memory!!\n",
      "!!!!!  [TODO]   !!!!! progress bar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4355, 4)\n",
      "Nr. Trainingsamples: 4355\n",
      "\n",
      "###################################\n",
      "#           Build Network          #\n",
      "###################################\n",
      "\n",
      "(64, 4, 4, 1024)input shape\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-93e4b6bc926f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3e93cf71245e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, start_run)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3e93cf71245e>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m###### Build Generator####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m####### Placeholders for Images####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3e93cf71245e>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m#welche kernel größe??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m#deconv_layer(input, target_shape, filter, strides, padding, bias_init, norm_axes=[0,1,2], normalize=False, activation=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0md_second_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_first\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SAME\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0msecond_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_second_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m#print(np.shape(second_layer))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3e93cf71245e>\u001b[0m in \u001b[0;36mdeconv_layer\u001b[0;34m(input, target_shape, filter, strides, padding, bias_init, norm_axes, normalize, activation)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mist_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mconv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dl = DataLoader('data', load_into_memory=True)\n",
    "    #gen_test = dl.get_batches(32)\n",
    "    #print(type(next(gen_test)))\n",
    "    #for  one,two,three in \n",
    "    #batch,wrong_batch=next(gen_test)\n",
    "    #batch[0]\n",
    "    \n",
    "    \n",
    "    net = GAN()\n",
    "    train(net, gen = dl, epochs = 2000, restore =Settings.restore)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################################\n",
      "#          Start Training         #\n",
      "###################################\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't load save_path when it is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a65c1213a705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-32420ff29d95>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, gen, epochs, restore)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#new_saver = tf.train.import_meta_graph('./stored-1.meta')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m#new_saver=tf.train.import_meta_graph(\"./store_weights/\"+str(Settings.run-1)+\"/stored.meta\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./store_weights/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;31m#saver.restore(session, tf.train.latest_checkpoint('./store_weights/'+str(507)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#saver.restore(session, \"./horse/stored.meta\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1680\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't load save_path when it is None."
     ]
    }
   ],
   "source": [
    "train(net,dl,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dl=DataLoader('.')\n",
    "gen_test = dl.get_batches(100)\n",
    "batch,wrong_batch=next(gen_test)\n",
    "print(np.shape(batch[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n",
      "(99,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(batch[:,0][0]))\n",
    "a=batch[:,0]\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=batch[0:32]\n",
    "print(np.shape(a))\n",
    "################################need this to get rid of embediings!!!!!!!!!!!!!\n",
    "a=np.delete(a, 0, axis=1)\n",
    "print(np.shape(a))\n",
    "print(np.shape(a[1][0]))\n",
    "print(np.shape(generated_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jolan/anaconda/envs/tensorflow/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_embeddings: (10, 1024)\n",
      "real_images: (10, 64, 64, 3)\n",
      "wrong_embeddings: (10, 1024)\n",
      "wrong_images: (10, 64, 64, 3)\n",
      "random_embeddings: (10, 1024)\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader('data')\n",
    "generator = data_loader.get_batches(10)\n",
    "real_embeddings, real_images, wrong_embeddings, wrong_images, random_embeddings = next(generator)\n",
    "\n",
    "\n",
    "print(\"real_embeddings: \" + str(np.shape(real_embeddings)))\n",
    "print(\"real_images: \" + str(np.shape(real_images)))\n",
    "print(\"wrong_embeddings: \" + str(np.shape(wrong_embeddings)))\n",
    "print(\"wrong_images: \" + str(np.shape(wrong_images)))\n",
    "print(\"random_embeddings: \" + str(np.shape(random_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "ja = [i for i in range(30)]\n",
    "print(ja[0:10])\n",
    "print(ja[10:10*2])\n",
    "print(ja[10*2:10*3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch #1 in  0m 0.30s.\n"
     ]
    }
   ],
   "source": [
    "#always feed real captions\n",
    "print((\"Finished Epoch #{0} in {1: .0f}m{2: .2f}s.\").format(1, 0.2,0.3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a3a8f6cd61e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     image_file =  join(data_dir, 'flowers/jpg/'+loaded_data['image_list'][idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     image_array = image_processing.load_image_array(image_file, image_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_data' is not defined"
     ]
    }
   ],
   "source": [
    "batch_no = 1\n",
    "batch_size = 64\n",
    "real_images = np.zeros((batch_size, 64, 64, 3))\n",
    "wrong_images = np.zeros((batch_size, 64, 64, 3))\n",
    "captions = np.zeros((batch_size, 1024))\n",
    "\n",
    "cnt = 0\n",
    "image_files = []\n",
    "for i in range(batch_no * batch_size, batch_no * batch_size + batch_size):\n",
    "    idx = i % len(loaded_data['image_list'])\n",
    "    image_file =  join(data_dir, 'flowers/jpg/'+loaded_data['image_list'][idx])\n",
    "    image_array = image_processing.load_image_array(image_file, image_size)\n",
    "    real_images[cnt,:,:,:] = image_array\n",
    "\n",
    "    # Improve this selection of wrong image\n",
    "    wrong_image_id = random.randint(0,len(loaded_data['image_list'])-1)\n",
    "    wrong_image_file =  join(data_dir, 'flowers/jpg/'+loaded_data['image_list'][wrong_image_id])\n",
    "    wrong_image_array = image_processing.load_image_array(wrong_image_file, image_size)\n",
    "    wrong_images[cnt, :,:,:] = wrong_image_array\n",
    "\n",
    "    random_caption = random.randint(0,4)\n",
    "    captions[cnt,:] = loaded_data['captions'][ loaded_data['image_list'][idx] ][ random_caption ][0:caption_vector_length]\n",
    "    image_files.append( image_file )\n",
    "    cnt += 1\n",
    "\n",
    "z_noise = np.random.uniform(-1, 1, [batch_size, z_dim])\n",
    "return real_images, wrong_images, captions, z_noise, image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "hello=[1,2,3,4]\n",
    "print(hello[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-test",
   "language": "python",
   "name": "py36-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
